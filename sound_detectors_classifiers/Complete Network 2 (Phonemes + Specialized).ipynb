{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Complete Network 2 (Phonemes + Specialized).ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"pQ0m9yaDqUnF"},"source":["# Frame to Phoneme Classifier\n","Model network 2, includes:\n","*    Pre-trained Phoneme Shallow Detectors (38)\n","*    Pre-trained Specialized Task Classifiers (10)\n","\n","Combines these models into a final linear layer, and outputs phoneme probabilities\n","\n"]},{"cell_type":"code","metadata":{"id":"lS7elo3dEOZK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1620171552650,"user_tz":240,"elapsed":18818,"user":{"displayName":"Marta Méndez Simón","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gits5ZctTuGucR808yQ0fvncHIaUqOjK19t0PujGQ=s64","userId":"15648474664899414572"}},"outputId":"5b85ae31-11f7-4500-eda4-1b13918346d3"},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"HuW97yYYrDNK","executionInfo":{"status":"ok","timestamp":1620171553766,"user_tz":240,"elapsed":1111,"user":{"displayName":"Marta Méndez Simón","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gits5ZctTuGucR808yQ0fvncHIaUqOjK19t0PujGQ=s64","userId":"15648474664899414572"}}},"source":["drivepath = '/content/gdrive/MyDrive/Spring_2021/11785_Intro_to_Deep_Learning/DL_Group_Project'\n","specialized_models_path = f'{drivepath}/experiments/specialized_detectors/models'\n","phoneme_models_path = f'{drivepath}/experiments/phoneme_detectors/'\n","datapath = '/content/gdrive/MyDrive/Spring_2021/11785_Intro_to_Deep_Learning/dl-group-project-code/preprocessed_data'\n","drivepath_final = f'{drivepath}/experiments/complete_network_2/'"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"tNu7076tIgbg","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1620171557367,"user_tz":240,"elapsed":4690,"user":{"displayName":"Marta Méndez Simón","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gits5ZctTuGucR808yQ0fvncHIaUqOjK19t0PujGQ=s64","userId":"15648474664899414572"}},"outputId":"4b17e3f5-0736-4883-bd66-3079a6ef6159"},"source":["!pip install tqdm"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.41.1)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"a0sSLZGvAcnu","executionInfo":{"status":"ok","timestamp":1620171560083,"user_tz":240,"elapsed":6179,"user":{"displayName":"Marta Méndez Simón","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gits5ZctTuGucR808yQ0fvncHIaUqOjK19t0PujGQ=s64","userId":"15648474664899414572"}}},"source":["import numpy as np\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","import os\n","from tqdm import tqdm\n","import time\n","from sklearn.metrics import classification_report\n","from sklearn.metrics import confusion_matrix\n","import pandas as pd"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"toPNjU3GMS7A","executionInfo":{"status":"ok","timestamp":1620171561003,"user_tz":240,"elapsed":915,"user":{"displayName":"Marta Méndez Simón","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gits5ZctTuGucR808yQ0fvncHIaUqOjK19t0PujGQ=s64","userId":"15648474664899414572"}}},"source":["NUM_EPOCHS = 100\n","BATCH_SIZE = 64\n","HIDDEN_SIZE_shallow = 128\n","HIDDEN_SIZE_spec = 128\n","MODEL_VERSION = 1\n","LEARNING_RATE = 0.01\n","OTHER_PHONEMES_PERCENT = 0.1"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"HlpeV6MJHvzv","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1620171561941,"user_tz":240,"elapsed":916,"user":{"displayName":"Marta Méndez Simón","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gits5ZctTuGucR808yQ0fvncHIaUqOjK19t0PujGQ=s64","userId":"15648474664899414572"}},"outputId":"ac30fc3c-3591-465b-dc26-bee50634a723"},"source":["cuda = torch.cuda.is_available()\n","num_workers = 8 if cuda else 0\n","DEVICE = \"cuda\" if cuda else \"cpu\"\n","print(\"Cuda = \"+str(cuda)+\" with num_workers = \"+str(num_workers))"],"execution_count":6,"outputs":[{"output_type":"stream","text":["Cuda = True with num_workers = 8\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"XnYfqte6xK-6","executionInfo":{"status":"ok","timestamp":1620171562186,"user_tz":240,"elapsed":747,"user":{"displayName":"Marta Méndez Simón","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gits5ZctTuGucR808yQ0fvncHIaUqOjK19t0PujGQ=s64","userId":"15648474664899414572"}}},"source":["class PhonemesDataset(Dataset):\n","    \n","    def __init__(self, basepath, mode):\n","      phoneme_features = np.zeros((1, 40))  # eliminate this row\n","      phoneme_labels = np.zeros((1))  # eliminate this row\n","\n","      with os.scandir(basepath) as entries:\n","        for entry in entries:\n","          if entry.is_file():\n","            if \"features\" in entry.name and mode in entry.name:\n","              phoneme_tag = entry.name.split(\"_\")[0]\n","\n","              features_filepath = entry.path\n","              labels_filepath = f\"{basepath}/{phoneme_tag}_{mode}_labels.npy\"\n","\n","              other_phoneme_features = np.load(features_filepath, allow_pickle=True)\n","              other_phoneme_labels = np.load(labels_filepath, allow_pickle=True)\n","              \n","              # stack to phoneme features\n","              phoneme_features = np.concatenate((phoneme_features, other_phoneme_features))\n","              phoneme_labels = np.concatenate((phoneme_labels, other_phoneme_labels))\n","              \n","      self.X = phoneme_features[1:]\n","      self.Y = phoneme_labels[1:]\n","\n","    def __len__(self):\n","        return len(self.X)\n"," \n","    # get a row at an index\n","    def __getitem__(self, index):\n","        x = torch.Tensor(self.X[index]).float()\n","        y = torch.as_tensor(self.Y[index]).long()\n","    \n","        return x,y"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"psP0nwMNE9yv","executionInfo":{"status":"ok","timestamp":1620171562186,"user_tz":240,"elapsed":301,"user":{"displayName":"Marta Méndez Simón","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gits5ZctTuGucR808yQ0fvncHIaUqOjK19t0PujGQ=s64","userId":"15648474664899414572"}}},"source":["def make_dataloader(dataset, train, batch_size):\n","  if train:\n","    shuffle = True\n","    drop_last = True\n","  else:\n","    shuffle = False\n","    drop_last = False\n","    \n","  loader = DataLoader(dataset=dataset, batch_size=batch_size,\n","                      drop_last=drop_last, shuffle=shuffle,\n","                      pin_memory=True, num_workers=8)\n","  \n","  return loader"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"--p_S9L8GWRd","executionInfo":{"status":"ok","timestamp":1620171567235,"user_tz":240,"elapsed":354,"user":{"displayName":"Marta Méndez Simón","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gits5ZctTuGucR808yQ0fvncHIaUqOjK19t0PujGQ=s64","userId":"15648474664899414572"}}},"source":["class PhonemeShallowDetector(nn.Module):\n","  \n","  def __init__(self, hidden_size, activation):\n","    super(PhonemeShallowDetector, self).__init__()\n","    \n","    self.linear_layer = nn.Linear(in_features=40, out_features=hidden_size)\n","    self.bn_layer = nn.BatchNorm1d(num_features=hidden_size)\n","    self.activation = activation\n","    self.output_layer = nn.Linear(in_features=hidden_size, out_features=1)\n","    self.sigmoid = nn.Sigmoid()\n","    seq_params = [\n","      self.linear_layer,\n","      self.bn_layer,\n","      self.activation,\n","      self.output_layer,\n","      self.sigmoid\n","    ]\n","\n","    self.network = nn.Sequential(*seq_params)\n","    \n","  def forward(self, x):\n","    return self.network(x)"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"DAI4xDfb6XU-","executionInfo":{"status":"ok","timestamp":1620171820650,"user_tz":240,"elapsed":457,"user":{"displayName":"Marta Méndez Simón","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gits5ZctTuGucR808yQ0fvncHIaUqOjK19t0PujGQ=s64","userId":"15648474664899414572"}}},"source":["class SpecializedShallowDetector(nn.Module):\n","  \n","  def __init__(self, hidden_size, activation):\n","    super(SpecializedShallowDetector, self).__init__()\n","    \n","    self.linear_layer = nn.Linear(in_features=40, out_features=hidden_size)\n","    self.bn_layer = nn.BatchNorm1d(num_features=hidden_size)\n","    self.activation = activation\n","    self.output_layer = nn.Linear(in_features=hidden_size, out_features=1)\n","    self.sigmoid = nn.Sigmoid()\n","    seq_params = [\n","      self.linear_layer,\n","      self.bn_layer,\n","      self.activation,\n","      self.output_layer,\n","      self.sigmoid\n","    ]\n","\n","    self.network = nn.Sequential(*seq_params)\n","    \n","  def forward(self, x):\n","    return self.network(x)"],"execution_count":20,"outputs":[]},{"cell_type":"code","metadata":{"id":"Z65FZ5qdK5El","executionInfo":{"status":"ok","timestamp":1620171863498,"user_tz":240,"elapsed":859,"user":{"displayName":"Marta Méndez Simón","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gits5ZctTuGucR808yQ0fvncHIaUqOjK19t0PujGQ=s64","userId":"15648474664899414572"}}},"source":["initialized_shallow = []\n","initialized_specialized = []\n","\n","class FramePhonemeClassifierModel(nn.Module):\n","  \n","  def __init__(self, phoneme_mapper, specialized_mapper):\n","    super(FramePhonemeClassifierModel, self).__init__()\n","    \n","    self.phoneme_mapper = phoneme_mapper\n","    self.specialized_mapper = specialized_mapper\n","\n","    # generate the SpecializedShallowDetector \n","    specialized_detectors = []\n","    for spec_idx, spec_type in enumerate(self.specialized_mapper):\n","      specialized_detector = SpecializedShallowDetector(hidden_size=HIDDEN_SIZE_spec, \n","                                                        activation=nn.LeakyReLU()).to(DEVICE)\n","      specialized_detectors.append(specialized_detector)\n","\n","    self.specialized_detectors = nn.ModuleList(specialized_detectors)\n","\n","    # generate the PhonemeShallowDetectors \n","    phoneme_detectors = []\n","    for phoneme_index, phoneme_tag in phoneme_mapper.items():\n","      phoneme_detector = PhonemeShallowDetector(hidden_size=HIDDEN_SIZE_shallow, \n","                                                activation=nn.LeakyReLU())\n","      phoneme_detectors.append(phoneme_detector)\n","\n","    self.phoneme_detectors = nn.ModuleList(phoneme_detectors)\n","\n","    self.linear_layer = nn.Linear(in_features=len(phoneme_mapper)+len(specialized_mapper), \n","                                  out_features=len(phoneme_mapper))\n","\n","    self.initialize_specialized_detectors()\n","    self.initialize_shallow_detectors()\n","\n","  def initialize_shallow_detectors(self):\n","    # load weights from shallow detectors pre-trained models\n","    for phoneme_index, phoneme_tag in self.phoneme_mapper.items():\n","      phoneme_shallow_detector = self.phoneme_detectors[phoneme_index]\n","\n","      phoneme_model_path = f\"{phoneme_models_path}/model_{phoneme_tag}_{MODEL_VERSION}_99\"\n","      temp = torch.load(phoneme_model_path)\n","      phoneme_shallow_detector.load_state_dict(temp['model_state_dict'])\n","      initialized_shallow.append(temp['model_state_dict'])\n","  \n","  def initialize_specialized_detectors(self):\n","    # load weights from shallow detectors pre-trained models\n","     for spec_idx, spec_type in enumerate(self.specialized_mapper):\n","      specialized_detector = self.specialized_detectors[spec_idx]\n","\n","      spec_model_path = f\"{specialized_models_path}/model_{spec_type}_{MODEL_VERSION}_29\"\n","      temp = torch.load(spec_model_path)\n","      specialized_detector.load_state_dict(temp['model_state_dict'])\n","      initialized_specialized.append(temp['model_state_dict'])\n","  \n","  def forward(self, x):\n","    \"\"\"\n","    Returns tuple:\n","      final outputs (B, NUM_PHONEMES) and shallow classifier outputs (B, NUM_SHALLOW_DETECTORS)\n","    \"\"\"\n","    shallow_classifier_outputs = []\n","    for phoneme_index, phoneme_tag in self.phoneme_mapper.items():\n","      phoneme_shallow_detector = self.phoneme_detectors[phoneme_index]\n","      # run frame through shallow detector\n","      output = phoneme_shallow_detector(x)\n","      shallow_classifier_outputs.append(output.reshape(-1))\n","    \n","    for spec_idx, spec_type in enumerate(self.specialized_mapper):\n","      # go through specialized\n","      specialized_detector = self.specialized_detectors[spec_idx]\n","      output = specialized_detector(x)\n","      shallow_classifier_outputs.append(output.reshape(-1))\n","\n","    # convert to torch tensor\n","    shallow_classifier_outputs = torch.vstack(shallow_classifier_outputs).T\n","    outputs = self.linear_layer(shallow_classifier_outputs)\n","\n","    self.initialize_specialized_detectors()\n","    self.initialize_shallow_detectors()\n","\n","    return outputs, shallow_classifier_outputs"],"execution_count":29,"outputs":[]},{"cell_type":"code","metadata":{"id":"KNhuSUg0_5sH","executionInfo":{"status":"ok","timestamp":1620171863762,"user_tz":240,"elapsed":863,"user":{"displayName":"Marta Méndez Simón","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gits5ZctTuGucR808yQ0fvncHIaUqOjK19t0PujGQ=s64","userId":"15648474664899414572"}}},"source":["class FramePhonemeClassifier():\n","\n","  def __init__(self, phoneme_mapper, specialized_mapper):\n","\n","    train_data = PhonemesDataset(basepath=datapath, mode=\"train\")\n","    self.train_loader = make_dataloader(dataset=train_data, train=True, batch_size=BATCH_SIZE)\n","    print(f\"train_data.shape: {train_data.X.shape}\")\n","\n","    dev_data = PhonemesDataset(basepath=datapath, mode=\"dev\")\n","    self.dev_loader = make_dataloader(dataset=dev_data, train=False, batch_size=BATCH_SIZE)\n","    print(f\"dev_data.shape: {dev_data.X.shape}\")\n","    \n","    self.model = FramePhonemeClassifierModel(phoneme_mapper, specialized_mapper).to(DEVICE)\n","\n","    self.criterion = nn.CrossEntropyLoss()\n","    self.optimizer = torch.optim.SGD(self.model.parameters(), lr=LEARNING_RATE, momentum=0.9)\n","    self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, 'min')\n","\n","    self.train_loss_per_epoch = []\n","    self.train_acc_per_epoch = []\n","    self.dev_loss_per_epoch = []\n","    self.dev_acc_per_epoch = []\n","  \n","  def save_model(self, epoch):\n","    model_epoch_path = f\"{drivepath_final}/models/model_{MODEL_VERSION}_{epoch}\"\n","    torch.save({\n","        'model_state_dict': self.model.state_dict(),\n","        'optimizer_state_dict': self.optimizer.state_dict(),\n","        'scheduler_state_dict': self.scheduler.state_dict(),\n","    }, model_epoch_path)\n","    # print('saved model: {}'.format(model_epoch_path))\n","\n","  def train(self, epochs):\n","    # Run training and track with wandb\n","    total_batches = len(self.train_loader) * epochs\n","    example_ct = 0  # number of examples seen\n","    batch_ct = 0\n","\n","    for epoch in tqdm(range(epochs)):\n","        train_loss = 0.0\n","        start_time = time.time()\n","        total_predictions = 0\n","        correct_predictions = 0\n","\n","        true_labels = []\n","        predictions = []\n","        for _, (features, targets) in enumerate(self.train_loader):\n","            batch_loss, outputs = self.train_batch(features, targets)\n","            train_loss += batch_loss\n","\n","            example_ct += len(features)\n","            batch_ct += 1\n","\n","            # check number of correct predictions\n","            output_classes = torch.argmax(outputs.log_softmax(1), dim=1).detach().cpu()  # convert to class labels\n","            total_predictions += len(output_classes)\n","            correct_predictions += torch.sum(targets == output_classes)\n","\n","            true_labels += list(targets)\n","            predictions += list(output_classes)\n","\n","        end_time = time.time()\n","\n","        train_loss /= example_ct\n","        print(f\"training loss: {train_loss}; time: {end_time - start_time}s\")\n","        \n","        if (epoch + 1) % 10 == 0 or epoch == (epochs - 1):\n","          report = classification_report(true_labels, predictions, output_dict=True)\n","          df = pd.DataFrame(report).transpose()\n","          df.to_csv(f\"{drivepath_final}/reports/reports_train_{MODEL_VERSION}_{epoch + 1}.csv\", index=False)\n","          self.save_model(epoch)\n","\n","        train_acc = (correct_predictions/total_predictions) * 100.0\n","        print(f\"training accuracy: {train_acc}%\")\n","\n","        self.train_loss_per_epoch.append(train_loss)\n","        self.train_acc_per_epoch.append(train_acc)\n","\n","        # evaluate model with validation data\n","        dev_loss, dev_acc = self.evaluate_model(epoch)\n","        \n","        self.dev_loss_per_epoch.append(dev_loss)\n","        self.dev_acc_per_epoch.append(dev_acc)\n","\n","        # Step with the scheduler\n","        self.scheduler.step(dev_loss)\n","      \n","    # epoch completed, save model\n","    self.save_model(epoch)\n","\n","  def train_batch(self, features, targets):\n","    features, targets = features.to(DEVICE), targets.to(DEVICE)\n","    targets = targets.reshape(-1, 1)\n","\n","    self.optimizer.zero_grad()\n","\n","    # Forward pass ➡\n","    outputs, shallow_classifier_outputs = self.model(features)\n","    loss = self.criterion(outputs, targets.reshape(-1))  # compare with target outputs\n","\n","    # Backward pass ⬅\n","    loss.backward()\n","    \n","    # Step with optimizer\n","    self.optimizer.step()\n","\n","    return loss.item(), outputs\n","\n","  def evaluate_model(self, epoch, generate_graphs=False):\n","    with torch.no_grad():\n","      self.model.eval()\n","\n","      running_loss = 0.0\n","      total_predictions = 0.0\n","      correct_predictions = 0.0\n","\n","      true_labels = []\n","      predictions = []\n","\n","      example_ct = 0\n","      start_time = time.time()\n","      for batch_idx, (features, targets) in enumerate(self.dev_loader):\n","        features = features.to(DEVICE)\n","        targets = targets.to(DEVICE)\n","        targets = targets.reshape(-1, 1)\n","\n","        example_ct += len(features)\n","\n","        outputs, shallow_classifier_outputs = self.model(features)\n","        outputs = outputs.to(DEVICE)\n","\n","        # check number of correct predictions\n","        output_classes = torch.argmax(outputs.log_softmax(1), dim=1)  # convert to class labels\n","        total_predictions += len(output_classes)\n","        correct_predictions += torch.sum(targets.reshape(-1) == output_classes)\n","\n","        loss = self.criterion(outputs, targets.reshape(-1)).detach()\n","        running_loss += loss.item()\n","\n","        true_labels += list(targets.detach().cpu())\n","        predictions += list(output_classes.detach().cpu())\n","      \n","      end_time = time.time()\n","\n","      running_loss /= example_ct\n","      print(f\"testing loss: {running_loss}; time: {end_time - start_time}s\")\n","      acc = (correct_predictions/total_predictions) * 100.0\n","      print(f\"testing accuracy: {acc}%\")\n","\n","      if (epoch + 1) % 10 == 0:\n","        report = classification_report(true_labels, predictions, output_dict=True)\n","        df = pd.DataFrame(report).transpose()\n","        df.to_csv(f\"{drivepath_final}/reports/reports_dev_{MODEL_VERSION}_{epoch + 1}.csv\", index=False)\n","  \n","      return running_loss, acc"],"execution_count":30,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hmgukhuSqx7q"},"source":["# Train classifier"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9RgaXKNzySdO","executionInfo":{"status":"ok","timestamp":1620171863762,"user_tz":240,"elapsed":490,"user":{"displayName":"Marta Méndez Simón","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gits5ZctTuGucR808yQ0fvncHIaUqOjK19t0PujGQ=s64","userId":"15648474664899414572"}},"outputId":"a0e93f14-a161-4e7d-d60b-55cd0df3ff06"},"source":["%cd /content/gdrive/MyDrive/Spring_2021/11785_Intro_to_Deep_Learning/DL_Group_Project/Dataset/Preprocessed_Data"],"execution_count":31,"outputs":[{"output_type":"stream","text":["/content/gdrive/MyDrive/Spring_2021/11785_Intro_to_Deep_Learning/DL_Group_Project/Dataset/Preprocessed_Data\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"tjbfaEh8yYL1","executionInfo":{"status":"ok","timestamp":1620171864180,"user_tz":240,"elapsed":747,"user":{"displayName":"Marta Méndez Simón","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gits5ZctTuGucR808yQ0fvncHIaUqOjK19t0PujGQ=s64","userId":"15648474664899414572"}}},"source":["from utilities import PHONEME_MAPPER\n","from utilities import SPECIALIZED_TASKS"],"execution_count":32,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"N7TH0JJh3QiH","executionInfo":{"status":"ok","timestamp":1620171864511,"user_tz":240,"elapsed":826,"user":{"displayName":"Marta Méndez Simón","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gits5ZctTuGucR808yQ0fvncHIaUqOjK19t0PujGQ=s64","userId":"15648474664899414572"}},"outputId":"33a693c2-784c-4e2b-83fa-f0809e78eb8a"},"source":["%cd /"],"execution_count":33,"outputs":[{"output_type":"stream","text":["/\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"M3TvLFZBygZ4","executionInfo":{"status":"ok","timestamp":1620171864512,"user_tz":240,"elapsed":420,"user":{"displayName":"Marta Méndez Simón","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gits5ZctTuGucR808yQ0fvncHIaUqOjK19t0PujGQ=s64","userId":"15648474664899414572"}},"outputId":"3cf610bd-81cf-450a-d666-4fc76f3ae2c7"},"source":["print(PHONEME_MAPPER)"],"execution_count":34,"outputs":[{"output_type":"stream","text":["{0: 'SIL', 1: 'AE', 2: 'AH', 3: 'AW', 4: 'AY', 5: 'B', 6: 'EH', 7: 'D', 8: 'DH', 9: 'EE', 10: 'FF', 11: 'G', 12: 'HH', 13: 'IH', 14: 'II', 15: 'J', 16: 'K', 17: 'LL', 18: 'MM', 19: 'NN', 20: 'OH', 21: 'OO', 22: 'OW', 23: 'OY', 24: 'P', 25: 'RR', 26: 'SH', 27: 'SS', 28: 'T', 29: 'TH', 30: 'UE', 31: 'UH', 32: 'VV', 33: 'WW', 34: 'YY', 35: 'ZZ', 36: 'CH', 37: 'ER', 38: 'NG'}\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OYQLXKv3Bmwb","executionInfo":{"status":"ok","timestamp":1620171865126,"user_tz":240,"elapsed":716,"user":{"displayName":"Marta Méndez Simón","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gits5ZctTuGucR808yQ0fvncHIaUqOjK19t0PujGQ=s64","userId":"15648474664899414572"}},"outputId":"03eff14e-022f-42bb-bf48-832cd23759d4"},"source":["print(SPECIALIZED_TASKS)"],"execution_count":35,"outputs":[{"output_type":"stream","text":["{'1_vowel_vs_consonant': {0: ['EE', 'IH', 'EH', 'AE', 'UH', 'ER', 'AH', 'AW', 'OO', 'UE'], 1: ['FF', 'HH', 'MM', 'NN', 'NG', 'RR', 'SS', 'SH', 'VV', 'WW', 'YY', 'ZZ']}, '3_highvowel_vs_lowvowel': {0: ['EE', 'IH', 'UE', 'OO'], 1: ['AE', 'AH', 'AW']}, '4_voiced_vs_unvoiced_fricatives': {0: ['DH', 'VV', 'ZZ'], 1: ['FF', 'SS', 'SH', 'TH']}, '5_ss_vs_zz': {0: ['SS'], 1: ['ZZ']}, '6_b_vs_p': {0: ['B'], 1: ['P']}, '7_dh_vs_th': {0: ['DH'], 1: ['TH']}, '8_ww_vs_yy': {0: ['WW'], 1: ['YY']}, '9_ee_vs_aw': {0: ['EE'], 1: ['AW']}, '10_ah_vs_aw': {0: ['AH'], 1: ['AW']}, '11_mm_vs_nn': {0: ['MM'], 1: ['NN']}}\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"mvRk8m8CLwGn","colab":{"base_uri":"https://localhost:8080/","height":446},"executionInfo":{"status":"error","timestamp":1620171866455,"user_tz":240,"elapsed":1640,"user":{"displayName":"Marta Méndez Simón","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gits5ZctTuGucR808yQ0fvncHIaUqOjK19t0PujGQ=s64","userId":"15648474664899414572"}},"outputId":"c2c6fa6c-807b-485c-d427-b38e6261e999"},"source":["classifier = FramePhonemeClassifier(PHONEME_MAPPER, SPECIALIZED_TASKS)\n","classifier.train(epochs=NUM_EPOCHS)\n"],"execution_count":36,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"],"name":"stderr"},{"output_type":"stream","text":["train_data.shape: (54579, 40)\n","dev_data.shape: (11712, 40)\n"],"name":"stdout"},{"output_type":"stream","text":["  0%|          | 0/100 [00:00<?, ?it/s]\n"],"name":"stderr"},{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-36-6b3eefb053a4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mclassifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFramePhonemeClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPHONEME_MAPPER\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSPECIALIZED_TASKS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNUM_EPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-30-2dc3527c3a0a>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, epochs)\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m             \u001b[0mbatch_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m             \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbatch_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-30-2dc3527c3a0a>\u001b[0m in \u001b[0;36mtrain_batch\u001b[0;34m(self, features, targets)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0;31m# Backward pass ⬅\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;31m# Step with optimizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    145\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.cuda.FloatTensor [128, 1]], which is output 0 of TBackward, is at version 5; expected version 3 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True)."]}]},{"cell_type":"code","metadata":{"id":"z__2guLawLaG"},"source":[""],"execution_count":null,"outputs":[]}]}