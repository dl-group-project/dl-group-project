{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Complete_Network.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pQ0m9yaDqUnF"
      },
      "source": [
        "# Frame to Phoneme Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lS7elo3dEOZK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61975317-bfa1-4e64-c5f9-1f58fd7a1680"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bJQF8XwhESvy"
      },
      "source": [
        "drivepath_shallow = '/content/gdrive/MyDrive/DL_Group_Project/Dataset/Preprocessed_Data'"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zBC6Ip5v-EUH"
      },
      "source": [
        "drivepath_spec = '/content/gdrive/MyDrive/DL_Group_Project/experiments/specialized_detectors/models'"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tNu7076tIgbg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77887f71-b3a6-4712-891d-ebab24fa96ef"
      },
      "source": [
        "!pip install tqdm"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.41.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a0sSLZGvAcnu"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import pandas as pd"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "toPNjU3GMS7A"
      },
      "source": [
        "NUM_EPOCHS = 100\n",
        "BATCH_SIZE = 64\n",
        "HIDDEN_SIZE_shallow = 128\n",
        "HIDDEN_SIZE_spec = 128\n",
        "MODEL_VERSION = 1\n",
        "LEARNING_RATE = 0.01\n",
        "LOGISTIC_THRESHOLD = 0.5\n",
        "OTHER_PHONEMES_PERCENT = 0.1"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HlpeV6MJHvzv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2c20fe8-1649-4467-91cb-8fd82ab99f7b"
      },
      "source": [
        "cuda = torch.cuda.is_available()\n",
        "num_workers = 8 if cuda else 0\n",
        "DEVICE = \"cuda\" if cuda else \"cpu\"\n",
        "print(\"Cuda = \"+str(cuda)+\" with num_workers = \"+str(num_workers))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cuda = True with num_workers = 8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XnYfqte6xK-6"
      },
      "source": [
        "class PhonemesDataset(Dataset):\n",
        "    \n",
        "    def __init__(self, basepath, mode):\n",
        "      phoneme_features = np.zeros((1, 40))  # eliminate this row\n",
        "      phoneme_labels = np.zeros((1))  # eliminate this row\n",
        "\n",
        "      with os.scandir(basepath) as entries:\n",
        "        for entry in entries:\n",
        "          if entry.is_file():\n",
        "            if \"features\" in entry.name and mode in entry.name:\n",
        "              phoneme_tag = entry.name.split(\"_\")[0]\n",
        "\n",
        "              features_filepath = entry.path\n",
        "              labels_filepath = f\"{basepath}/{phoneme_tag}_{mode}_labels.npy\"\n",
        "\n",
        "              other_phoneme_features = np.load(features_filepath, allow_pickle=True)\n",
        "              other_phoneme_labels = np.load(labels_filepath, allow_pickle=True)\n",
        "              \n",
        "              # stack to phoneme features\n",
        "              phoneme_features = np.concatenate((phoneme_features, other_phoneme_features))\n",
        "              phoneme_labels = np.concatenate((phoneme_labels, other_phoneme_labels))\n",
        "              \n",
        "      self.X = phoneme_features[1:]\n",
        "      self.Y = phoneme_labels[1:]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        " \n",
        "    # get a row at an index\n",
        "    def __getitem__(self, index):\n",
        "        x = torch.Tensor(self.X[index]).float()\n",
        "        y = torch.as_tensor(self.Y[index]).long()\n",
        "    \n",
        "        return x,y"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kAY1D8WK6PpK"
      },
      "source": [
        "class SpecializedDataset(Dataset):\n",
        "    \n",
        "    def __init__(self, datapath, mode, task_name, phonemes_class_0, phonemes_class_1):\n",
        "      \"\"\"\n",
        "      phonemes_class_0: list of phoneme names for class 0\n",
        "      phonemes_class_1: list of phoneme names for class 1\n",
        "      \"\"\"\n",
        "      complete_features = np.zeros((1, 40))  # eliminate this row\n",
        "      complete_labels = np.zeros((1))  # eliminate this row\n",
        "\n",
        "      # go through all files in datapath, check phoneme if class=0 or class=1\n",
        "      # assign that label and discard silence frames\n",
        "      class_0_phonemes_found = []\n",
        "      class_1_phonemes_found = []\n",
        "      with os.scandir(datapath) as entries:\n",
        "        for entry in entries:\n",
        "          if entry.is_file():\n",
        "            if \"features\" in entry.name and mode in entry.name:\n",
        "              phoneme_tag = entry.name.split(\"_\")[0]\n",
        "\n",
        "              # find phoneme in class_0 or class_1 list and assign label\n",
        "              phoneme_class = None\n",
        "              if phoneme_tag in phonemes_class_0:\n",
        "                phoneme_class = 0\n",
        "                class_0_phonemes_found.append(phoneme_tag)\n",
        "              if phoneme_tag in phonemes_class_1:\n",
        "                phoneme_class = 1\n",
        "                class_1_phonemes_found.append(phoneme_tag)\n",
        "\n",
        "              if phoneme_class is None:\n",
        "                print(f\"phoneme '{phoneme_tag}' not found on class 0 nor class 1 lists; skip\") if DEBUG else None\n",
        "                continue\n",
        "\n",
        "              print(f\"phoneme '{phoneme_tag}' is class: {phoneme_class}\") if DEBUG else None\n",
        "\n",
        "              features_filepath = entry.path\n",
        "              labels_filepath = f\"{datapath}/{phoneme_tag}_{mode}_labels.npy\"\n",
        "\n",
        "              phoneme_features = np.load(features_filepath, allow_pickle=True)\n",
        "              phoneme_labels = np.load(labels_filepath, allow_pickle=True)\n",
        "              print(f\"{phoneme_tag} total features: {phoneme_features.shape}\") if DEBUG else None\n",
        "              print(f\"{phoneme_tag} total labels: {phoneme_labels.shape}\") if DEBUG else None\n",
        "\n",
        "              # find frames where label != 0 (non-silence)\n",
        "              non_zero_indexes = phoneme_labels.nonzero()\n",
        "              phoneme_features = phoneme_features[non_zero_indexes]\n",
        "              phoneme_labels = phoneme_labels[non_zero_indexes]\n",
        "              print(f\"{phoneme_tag} no-silence features: {phoneme_features.shape}\") if DEBUG else None\n",
        "              print(f\"{phoneme_tag} no-silence labels: {phoneme_labels.shape}\") if DEBUG else None\n",
        "\n",
        "              phoneme_labels[:] = phoneme_class  # label=class\n",
        "\n",
        "              # stack to phoneme features\n",
        "              complete_features = np.concatenate((complete_features, phoneme_features))\n",
        "              complete_labels = np.concatenate((complete_labels, phoneme_labels))\n",
        "      \n",
        "      self.X = complete_features[1:]\n",
        "      self.Y = complete_labels[1:]\n",
        "      print(f\"[task={task_name}] {self.X.shape} features\")\n",
        "      print(f\"[task={task_name}] {self.Y.shape} labels\")\n",
        "\n",
        "      if sorted(class_0_phonemes_found) != sorted(phonemes_class_0):\n",
        "        raise Exception(f\"class 0 phonemes found ({sorted(class_0_phonemes_found)}) != expected phonemes ({sorted(phonemes_class_0)})\")\n",
        " \n",
        "      if sorted(class_1_phonemes_found) != sorted(phonemes_class_1):\n",
        "        raise Exception(f\"class 1 phonemes found ({sorted(class_1_phonemes_found)}) != expected phonemes ({sorted(phonemes_class_1)})\")\n",
        " \n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        " \n",
        "    # get a row at an index\n",
        "    def __getitem__(self, index):\n",
        "        x = torch.Tensor(self.X[index]).float()\n",
        "        y = torch.as_tensor(self.Y[index]).float()\n",
        "    \n",
        "        return x,y"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "psP0nwMNE9yv"
      },
      "source": [
        "def make_dataloader(dataset, train, batch_size):\n",
        "  if train:\n",
        "    shuffle = True\n",
        "    drop_last = True\n",
        "  else:\n",
        "    shuffle = False\n",
        "    drop_last = False\n",
        "    \n",
        "  loader = DataLoader(dataset=dataset, batch_size=batch_size,\n",
        "                      drop_last=drop_last, shuffle=shuffle,\n",
        "                      pin_memory=True, num_workers=8)\n",
        "  \n",
        "  return loader"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "--p_S9L8GWRd"
      },
      "source": [
        "class PhonemeShallowDetector(nn.Module):\n",
        "  \n",
        "  def __init__(self, hidden_size, activation):\n",
        "    super(PhonemeShallowDetector, self).__init__()\n",
        "    \n",
        "    self.linear_layer = nn.Linear(in_features=40, out_features=hidden_size)\n",
        "    self.bn_layer = nn.BatchNorm1d(num_features=hidden_size)\n",
        "    self.activation = activation\n",
        "    self.output_layer = nn.Linear(in_features=hidden_size, out_features=1)\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "    seq_params = [\n",
        "      self.linear_layer,\n",
        "      self.bn_layer,\n",
        "      self.activation,\n",
        "      self.output_layer,\n",
        "      self.sigmoid\n",
        "    ]\n",
        "\n",
        "    self.network = nn.Sequential(*seq_params)\n",
        "    \n",
        "  def forward(self, x):\n",
        "    return self.network(x)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DAI4xDfb6XU-"
      },
      "source": [
        "class SpecializedShallowDetector(nn.Module):\n",
        "  \n",
        "  def __init__(self, hidden_size, activation):\n",
        "    super(SpecializedShallowDetector, self).__init__()\n",
        "    \n",
        "    self.linear_layer = nn.Linear(in_features=40, out_features=hidden_size)\n",
        "    self.bn_layer = nn.BatchNorm1d(num_features=hidden_size)\n",
        "    self.activation = activation\n",
        "    self.output_layer = nn.Linear(in_features=hidden_size, out_features=1)\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "    seq_params = [\n",
        "      self.linear_layer,\n",
        "      self.bn_layer,\n",
        "      self.activation,\n",
        "      self.output_layer,\n",
        "      self.sigmoid\n",
        "    ]\n",
        "\n",
        "    self.network = nn.Sequential(*seq_params)\n",
        "    \n",
        "  def forward(self, x):\n",
        "    return self.network(x)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z65FZ5qdK5El"
      },
      "source": [
        "class FramePhonemeClassifierModel(nn.Module):\n",
        "  \n",
        "  def __init__(self, phoneme_mapper, specialized_mapper):\n",
        "    super(FramePhonemeClassifierModel, self).__init__()\n",
        "    \n",
        "    self.phoneme_mapper = phoneme_mapper\n",
        "    self.specialized_mapper = specialized_mapper\n",
        "\n",
        "    # specialized detectors\n",
        "\n",
        "    specialized_detectors = []\n",
        "    for spec_idx, spec_type in enumerate(SPECIALIZED_TASKS):\n",
        "      specialized_detector = SpecializedShallowDetector(hidden_size=HIDDEN_SIZE_spec, \n",
        "                                            activation=nn.LeakyReLU()).to(DEVICE)\n",
        "      specialized_detectors.append(specialized_detector)\n",
        "    \n",
        "    self.specialized_detectors = nn.ModuleList(specialized_detectors)\n",
        "\n",
        "\n",
        "    shallow_detectors = []\n",
        "    # generate the PhonemeShallowDetectors \n",
        "    for phoneme_index, phoneme_tag in phoneme_mapper.items():\n",
        "      shallow_detector = PhonemeShallowDetector(hidden_size=HIDDEN_SIZE_shallow, \n",
        "                                                activation=nn.LeakyReLU())\n",
        "      shallow_detectors.append(shallow_detector)\n",
        "\n",
        "    self.shallow_detectors = nn.ModuleList(shallow_detectors)\n",
        "\n",
        "    self.linear_layer = nn.Linear(in_features=len(phoneme_mapper), out_features=len(phoneme_mapper))\n",
        "\n",
        "    self.initialize_specialized_detectors()\n",
        "    self.initialize_shallow_detectors()\n",
        "\n",
        "  def initialize_shallow_detectors(self):\n",
        "    # load weights from shallow detectors pre-trained models\n",
        "    for phoneme_index, phoneme_tag in self.phoneme_mapper.items():\n",
        "      phoneme_shallow_detector = self.shallow_detectors[phoneme_index]\n",
        "\n",
        "      phoneme_model_path = f\"{drivepath_shallow}/shallow_detectors/model_{phoneme_tag}_{MODEL_VERSION}_99\"\n",
        "      temp = torch.load(phoneme_model_path)\n",
        "      phoneme_shallow_detector.load_state_dict(temp['model_state_dict'])\n",
        "  \n",
        "  def initialize_specialized_detectors(self):\n",
        "    # load weights from shallow detectors pre-trained models\n",
        "    for spec_idx, spec_type in self.phoneme_mapper.items():\n",
        "      specialized_detector = self.specialized_detectors[spec_idx]\n",
        "\n",
        "      spec_model_path = f\"{drivepath_spec}/model_{spec_type}_{MODEL_VERSION}_29\"\n",
        "      temp = torch.load(spec_model_path)\n",
        "      specialized_detector.load_state_dict(temp['model_state_dict'])\n",
        "  \n",
        "  def forward(self, x):\n",
        "    \n",
        "    # go through specialized\n",
        "    \n",
        "    shallow_outputs = []\n",
        "    for phoneme_index, phoneme_tag in self.phoneme_mapper.items():\n",
        "      phoneme_shallow_detector = self.shallow_detectors[phoneme_index]\n",
        "\n",
        "      # run frame through shallow detector\n",
        "      output = phoneme_shallow_detector(x)\n",
        "      shallow_outputs.append(output.reshape(-1))\n",
        "\n",
        "    # convert to torch tensor\n",
        "    shallow_outputs = torch.vstack(shallow_outputs).T\n",
        "    outputs = self.linear_layer(shallow_outputs)\n",
        "\n",
        "    return outputs"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KNhuSUg0_5sH"
      },
      "source": [
        "class FramePhonemeClassifier():\n",
        "\n",
        "  def __init__(self, phoneme_mapper, specialized_mapper):\n",
        "\n",
        "    train_data = PhonemesDataset(basepath=drivepath_shallow, mode=\"train\")\n",
        "    self.train_loader = make_dataloader(dataset=train_data, train=True, batch_size=BATCH_SIZE)\n",
        "    print(f\"train_data.shape: {train_data.X.shape}\")\n",
        "\n",
        "    dev_data = PhonemesDataset(basepath=drivepath_shallow, mode=\"dev\")\n",
        "    self.dev_loader = make_dataloader(dataset=dev_data, train=False, batch_size=BATCH_SIZE)\n",
        "    print(f\"dev_data.shape: {dev_data.X.shape}\")\n",
        "    \n",
        "    self.model = FramePhonemeClassifierModel(phoneme_mapper, specialized_mapper).to(DEVICE)\n",
        "\n",
        "    self.criterion = nn.CrossEntropyLoss()\n",
        "    self.optimizer = torch.optim.SGD(self.model.parameters(), lr=LEARNING_RATE, momentum=0.9)\n",
        "    self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, 'min')\n",
        "\n",
        "    self.train_loss_per_epoch = []\n",
        "    self.train_acc_per_epoch = []\n",
        "    self.dev_loss_per_epoch = []\n",
        "    self.dev_acc_per_epoch = []\n",
        "  \n",
        "  def save_model(self, epoch):\n",
        "    model_epoch_path = \"{}/complete_classifier/model_{}_{}\".format(drivepath,\n",
        "                                                                 MODEL_VERSION, \n",
        "                                                                 epoch)\n",
        "    torch.save({\n",
        "        'model_state_dict': self.model.state_dict(),\n",
        "        'optimizer_state_dict': self.optimizer.state_dict(),\n",
        "        'scheduler_state_dict': self.scheduler.state_dict(),\n",
        "    }, model_epoch_path)\n",
        "    # print('saved model: {}'.format(model_epoch_path))\n",
        "\n",
        "  def train(self, epochs):\n",
        "    # Run training and track with wandb\n",
        "    total_batches = len(self.train_loader) * epochs\n",
        "    example_ct = 0  # number of examples seen\n",
        "    batch_ct = 0\n",
        "\n",
        "    for epoch in tqdm(range(epochs)):\n",
        "        train_loss = 0.0\n",
        "        start_time = time.time()\n",
        "        total_predictions = 0\n",
        "        correct_predictions = 0\n",
        "\n",
        "        true_labels = []\n",
        "        predictions = []\n",
        "        for _, (features, targets) in enumerate(self.train_loader):\n",
        "            batch_loss, outputs = self.train_batch(features, targets)\n",
        "            train_loss += batch_loss\n",
        "\n",
        "            example_ct += len(features)\n",
        "            batch_ct += 1\n",
        "\n",
        "            # check number of correct predictions\n",
        "            output_classes = torch.argmax(outputs.log_softmax(1), dim=1).detach().cpu()  # convert to class labels\n",
        "            total_predictions += len(output_classes)\n",
        "            correct_predictions += torch.sum(targets == output_classes)\n",
        "\n",
        "            true_labels += list(targets)\n",
        "            predictions += list(output_classes)\n",
        "\n",
        "        end_time = time.time()\n",
        "\n",
        "        train_loss /= example_ct\n",
        "        print(f\"training loss: {train_loss}; time: {end_time - start_time}s\")\n",
        "        \n",
        "        if (epoch + 1) % 10 == 0 or epoch == (epochs - 1):\n",
        "          report = classification_report(true_labels, predictions, output_dict=True)\n",
        "          df = pd.DataFrame(report).transpose()\n",
        "          df.to_csv(f\"{drivepath}/final_classsifier/reports_train_{MODEL_VERSION}_{epoch + 1}.csv\", index=False)\n",
        "          self.save_model(epoch)\n",
        "\n",
        "        train_acc = (correct_predictions/total_predictions) * 100.0\n",
        "        print(f\"training accuracy: {train_acc}%\")\n",
        "\n",
        "        self.train_loss_per_epoch.append(train_loss)\n",
        "        self.train_acc_per_epoch.append(train_acc)\n",
        "\n",
        "        # evaluate model with validation data\n",
        "        dev_loss, dev_acc = self.evaluate_model(epoch)\n",
        "        \n",
        "        self.dev_loss_per_epoch.append(dev_loss)\n",
        "        self.dev_acc_per_epoch.append(dev_acc)\n",
        "\n",
        "        # Step with the scheduler\n",
        "        self.scheduler.step(dev_loss)\n",
        "      \n",
        "    # epoch completed, save model\n",
        "    self.save_model(epoch)\n",
        "\n",
        "  def train_batch(self, features, targets):\n",
        "    features, targets = features.to(DEVICE), targets.to(DEVICE)\n",
        "    targets = targets.reshape(-1, 1)\n",
        "\n",
        "    self.optimizer.zero_grad()\n",
        "\n",
        "    # Forward pass ➡\n",
        "    outputs = self.model(features)\n",
        "    loss = self.criterion(outputs, targets.reshape(-1))  # compare with target outputs\n",
        "    # Backward pass ⬅\n",
        "    loss.backward()\n",
        "    # Step with optimizer\n",
        "    self.optimizer.step()\n",
        "\n",
        "    return loss.item(), outputs\n",
        "\n",
        "  def evaluate_model(self, epoch):\n",
        "\n",
        "    with torch.no_grad():\n",
        "      self.model.eval()\n",
        "\n",
        "      running_loss = 0.0\n",
        "      total_predictions = 0.0\n",
        "      correct_predictions = 0.0\n",
        "\n",
        "      true_labels = []\n",
        "      predictions = []\n",
        "\n",
        "      example_ct = 0\n",
        "      start_time = time.time()\n",
        "      for batch_idx, (features, targets) in enumerate(self.dev_loader):\n",
        "        features = features.to(DEVICE)\n",
        "        targets = targets.to(DEVICE)\n",
        "        targets = targets.reshape(-1, 1)\n",
        "\n",
        "        example_ct += len(features)\n",
        "\n",
        "        outputs = self.model(features)\n",
        "        outputs = outputs.to(DEVICE)\n",
        "\n",
        "        # check number of correct predictions\n",
        "        output_classes = torch.argmax(outputs.log_softmax(1), dim=1)  # convert to class labels\n",
        "        total_predictions += len(output_classes)\n",
        "        correct_predictions += torch.sum(targets.reshape(-1) == output_classes)\n",
        "\n",
        "        loss = self.criterion(outputs, targets.reshape(-1)).detach()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        true_labels += list(targets.detach().cpu())\n",
        "        predictions += list(output_classes.detach().cpu())\n",
        "      \n",
        "      end_time = time.time()\n",
        "\n",
        "      running_loss /= example_ct\n",
        "      print(f\"testing loss: {running_loss}; time: {end_time - start_time}s\")\n",
        "      acc = (correct_predictions/total_predictions) * 100.0\n",
        "      print(f\"testing accuracy: {acc}%\")\n",
        "\n",
        "      if (epoch + 1) % 10 == 0:\n",
        "        report = classification_report(true_labels, predictions, output_dict=True)\n",
        "        df = pd.DataFrame(report).transpose()\n",
        "        df.to_csv(f\"{drivepath}/final_classsifier/reports_dev_{MODEL_VERSION}_{epoch + 1}.csv\", index=False)\n",
        "  \n",
        "      return running_loss, acc"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hmgukhuSqx7q"
      },
      "source": [
        "# Train classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9RgaXKNzySdO",
        "outputId": "83347870-320c-4202-ef21-622b1ebdf2b9"
      },
      "source": [
        "%cd /content/gdrive/MyDrive/DL_Group_Project/Dataset/Preprocessed_Data"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/.shortcut-targets-by-id/1qwJK2jyGMl2dPnVFe6JNZvrrG45HoonZ/DL_Group_Project/Dataset/Preprocessed_Data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tjbfaEh8yYL1"
      },
      "source": [
        "from utilities import PHONEME_MAPPER\n",
        "from utilities import SPECIALIZED_TASKS"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l6IVbR4w7We_"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N7TH0JJh3QiH",
        "outputId": "cb56c0ed-45d9-4d2c-cce9-a2b9e7ccb0f2"
      },
      "source": [
        "%cd /"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M3TvLFZBygZ4",
        "outputId": "168f277f-ee62-4525-fe22-e41709ce39d8"
      },
      "source": [
        "print(PHONEME_MAPPER)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{0: 'SIL', 1: 'AE', 2: 'AH', 3: 'AW', 4: 'AY', 5: 'B', 6: 'BIT', 7: 'D', 8: 'DH', 9: 'EE', 10: 'FF', 11: 'G', 12: 'HH', 13: 'IH', 14: 'II', 15: 'J', 16: 'K', 17: 'LL', 18: 'MM', 19: 'NN', 20: 'OH', 21: 'OO', 22: 'OW', 23: 'OY', 24: 'P', 25: 'RR', 26: 'SH', 27: 'SS', 28: 'T', 29: 'TH', 30: 'UE', 31: 'UH', 32: 'VV', 33: 'WW', 34: 'YY', 35: 'ZZ'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OYQLXKv3Bmwb",
        "outputId": "3df4c67f-d2c4-45d4-ecc2-2bf23d170f14"
      },
      "source": [
        "print(SPECIALIZED_TASKS)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'1_vowel_vs_consonant': {0: ['EE', 'IH', 'EH', 'AE', 'UH', 'ER', 'AH', 'AW', 'OO', 'UE'], 1: ['FF', 'HH', 'MM', 'NN', 'NG', 'RR', 'SS', 'SH', 'VV', 'WW', 'YY', 'ZZ']}, '3_highvowel_vs_lowvowel': {0: ['EE', 'IH', 'UE', 'OO'], 1: ['AE', 'AH', 'AW']}, '4_voiced_vs_unvoiced_fricatives': {0: ['DH', 'VV', 'ZZ'], 1: ['FF', 'SS', 'SH', 'TH']}, '5_ss_vs_zz': {0: ['SS'], 1: ['ZZ']}, '6_b_vs_p': {0: ['B'], 1: ['P']}, '7_dh_vs_th': {0: ['DH'], 1: ['TH']}, '8_ww_vs_yy': {0: ['WW'], 1: ['YY']}, '9_ee_vs_aw': {0: ['EE'], 1: ['AW']}, '10_ah_vs_aw': {0: ['AH'], 1: ['AW']}, '11_mm_vs_nn': {0: ['MM'], 1: ['NN']}}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mvRk8m8CLwGn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4cc66675-07b8-46af-f2e6-e7e8080d6783"
      },
      "source": [
        "classifier = FramePhonemeClassifier(PHONEME_MAPPER, SPECIALIZED_TASKS)\n",
        "classifier.train(epochs=NUM_EPOCHS)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "train_data.shape: (49976, 40)\n",
            "dev_data.shape: (10724, 40)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  0%|          | 0/100 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss: 0.026293274411597314; time: 17.92014217376709s\n",
            "training accuracy: 60.637020111083984%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  1%|          | 1/100 [00:19<32:19, 19.59s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "testing loss: 0.028341506538007384; time: 1.6592097282409668s\n",
            "testing accuracy: 57.627750396728516%\n",
            "training loss: 0.01059438670412279; time: 16.894349575042725s\n",
            "training accuracy: 70.30648803710938%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  2%|▏         | 2/100 [00:37<31:25, 19.23s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "testing loss: 0.023644539369537688; time: 1.4994187355041504s\n",
            "testing accuracy: 68.25811004638672%\n",
            "training loss: 0.005959935536066818; time: 16.883193016052246s\n",
            "training accuracy: 75.68910217285156%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  3%|▎         | 3/100 [00:56<30:46, 19.04s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "testing loss: 0.020977551176593803; time: 1.6836140155792236s\n",
            "testing accuracy: 71.34465026855469%\n",
            "training loss: 0.004016672528516023; time: 16.728933095932007s\n",
            "training accuracy: 78.18109130859375%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  4%|▍         | 4/100 [01:14<30:04, 18.80s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "testing loss: 0.019691348855384736; time: 1.5055067539215088s\n",
            "testing accuracy: 74.35658264160156%\n",
            "training loss: 0.002976094618415794; time: 16.619519233703613s\n",
            "training accuracy: 79.94591522216797%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  5%|▌         | 5/100 [01:33<29:32, 18.66s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "testing loss: 0.01889817395547307; time: 1.6868975162506104s\n",
            "testing accuracy: 74.8508071899414%\n",
            "training loss: 0.002346035377241862; time: 16.981719493865967s\n",
            "training accuracy: 80.92548370361328%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  6%|▌         | 6/100 [01:51<29:09, 18.61s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "testing loss: 0.018225480854811398; time: 1.5170307159423828s\n",
            "testing accuracy: 76.0257339477539%\n",
            "training loss: 0.0019377700736295882; time: 16.57915163040161s\n",
            "training accuracy: 81.7528076171875%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  7%|▋         | 7/100 [02:09<28:36, 18.45s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "testing loss: 0.017386962838557087; time: 1.486297845840454s\n",
            "testing accuracy: 76.45468139648438%\n",
            "training loss: 0.001632117870180175; time: 16.718029499053955s\n",
            "training accuracy: 82.5340576171875%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  8%|▊         | 8/100 [02:27<28:10, 18.37s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "testing loss: 0.017096040452907264; time: 1.4576747417449951s\n",
            "testing accuracy: 77.1260757446289%\n",
            "training loss: 0.001412103206325227; time: 16.773301362991333s\n",
            "training accuracy: 82.97676086425781%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  9%|▉         | 9/100 [02:46<27:48, 18.34s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "testing loss: 0.016770988544369314; time: 1.465083122253418s\n",
            "testing accuracy: 78.26370239257812%\n",
            "training loss: 0.0012355741031038073; time: 16.788450241088867s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "training accuracy: 83.5556869506836%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 10%|█         | 10/100 [03:05<28:03, 18.70s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "testing loss: 0.01632744227826084; time: 1.4573919773101807s\n",
            "testing accuracy: 78.28235626220703%\n",
            "training loss: 0.0011058059365256301; time: 16.75439429283142s\n",
            "training accuracy: 83.81009674072266%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 11%|█         | 11/100 [03:24<27:37, 18.62s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "testing loss: 0.015863287319936043; time: 1.6718213558197021s\n",
            "testing accuracy: 78.61805725097656%\n",
            "training loss: 0.000987828955000553; time: 16.62383508682251s\n",
            "training accuracy: 84.19271087646484%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 12%|█▏        | 12/100 [03:42<27:07, 18.50s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "testing loss: 0.01603064478883233; time: 1.5627880096435547s\n",
            "testing accuracy: 77.9932861328125%\n",
            "training loss: 0.0009008577972911372; time: 16.654590845108032s\n",
            "training accuracy: 84.385009765625%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 13%|█▎        | 13/100 [04:00<26:43, 18.43s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "testing loss: 0.01571180577939102; time: 1.6175179481506348s\n",
            "testing accuracy: 79.7650146484375%\n",
            "training loss: 0.0008226104624374964; time: 16.698410034179688s\n",
            "training accuracy: 84.6394271850586%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 14%|█▍        | 14/100 [04:18<26:18, 18.36s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "testing loss: 0.015784698440173987; time: 1.4704539775848389s\n",
            "testing accuracy: 78.50615692138672%\n",
            "training loss: 0.0007572538426352872; time: 16.82315969467163s\n",
            "training accuracy: 84.93789672851562%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 15%|█▌        | 15/100 [04:37<26:00, 18.36s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "testing loss: 0.016631874220750752; time: 1.5276532173156738s\n",
            "testing accuracy: 77.74151611328125%\n",
            "training loss: 0.0007011331536117583; time: 16.92078924179077s\n",
            "training accuracy: 85.0821304321289%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 16%|█▌        | 16/100 [04:55<25:43, 18.37s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "testing loss: 0.015037650419611043; time: 1.4587209224700928s\n",
            "testing accuracy: 79.75569152832031%\n",
            "training loss: 0.0006554265883203377; time: 16.694656133651733s\n",
            "training accuracy: 85.38060760498047%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 17%|█▋        | 17/100 [05:13<25:22, 18.34s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "testing loss: 0.01595822441871377; time: 1.5591380596160889s\n",
            "testing accuracy: 79.69041442871094%\n",
            "training loss: 0.000612076391244408; time: 16.924307584762573s\n",
            "training accuracy: 85.33052825927734%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 18%|█▊        | 18/100 [05:32<25:07, 18.38s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "testing loss: 0.014967180228542366; time: 1.5494422912597656s\n",
            "testing accuracy: 80.4830322265625%\n",
            "training loss: 0.000575239917228197; time: 16.671935081481934s\n",
            "training accuracy: 85.50680541992188%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 19%|█▉        | 19/100 [05:50<24:47, 18.37s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "testing loss: 0.014973872318635988; time: 1.6529953479766846s\n",
            "testing accuracy: 79.97948455810547%\n",
            "training loss: 0.0005382172934388598; time: 17.05228853225708s\n",
            "training accuracy: 85.67507934570312%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 20%|██        | 20/100 [06:10<24:56, 18.71s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "testing loss: 0.015479607552745724; time: 1.5276920795440674s\n",
            "testing accuracy: 80.1193618774414%\n",
            "training loss: 0.0005107157295956408; time: 16.64642024040222s\n",
            "training accuracy: 85.80529022216797%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 21%|██        | 21/100 [06:28<24:25, 18.55s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "testing loss: 0.015219622763017584; time: 1.5004088878631592s\n",
            "testing accuracy: 79.90489196777344%\n",
            "training loss: 0.0004835142929862429; time: 16.68054986000061s\n",
            "training accuracy: 85.99158477783203%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 22%|██▏       | 22/100 [06:46<23:58, 18.44s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "testing loss: 0.01458742088405155; time: 1.4967267513275146s\n",
            "testing accuracy: 80.35247802734375%\n",
            "training loss: 0.00045718554495091456; time: 16.73304510116577s\n",
            "training accuracy: 86.0556869506836%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 23%|██▎       | 23/100 [07:04<23:35, 18.38s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "testing loss: 0.014924306530854187; time: 1.4904823303222656s\n",
            "testing accuracy: 80.52033233642578%\n",
            "training loss: 0.0004361879275430145; time: 16.877310037612915s\n",
            "training accuracy: 86.1338119506836%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 24%|██▍       | 24/100 [07:23<23:18, 18.41s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "testing loss: 0.014099321217289703; time: 1.5791246891021729s\n",
            "testing accuracy: 81.22901916503906%\n",
            "training loss: 0.00041489737504758897; time: 16.78594422340393s\n",
            "training accuracy: 86.32612609863281%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 25%|██▌       | 25/100 [07:41<22:58, 18.37s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "testing loss: 0.014308764077153325; time: 1.5004889965057373s\n",
            "testing accuracy: 80.9585952758789%\n",
            "training loss: 0.0003962958672316286; time: 16.711018323898315s\n",
            "training accuracy: 86.29808044433594%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 26%|██▌       | 26/100 [07:59<22:37, 18.34s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "testing loss: 0.014944728906448766; time: 1.5379834175109863s\n",
            "testing accuracy: 80.66952514648438%\n",
            "training loss: 0.0003790121187999003; time: 16.640118837356567s\n",
            "training accuracy: 86.35617065429688%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 27%|██▋       | 27/100 [08:17<22:14, 18.28s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "testing loss: 0.014338642287914632; time: 1.4862592220306396s\n",
            "testing accuracy: 80.52033233642578%\n",
            "training loss: 0.00036263247291325515; time: 16.926597356796265s\n",
            "training accuracy: 86.50440216064453%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 28%|██▊       | 28/100 [08:36<22:00, 18.34s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "testing loss: 0.014348514130737122; time: 1.529355764389038s\n",
            "testing accuracy: 80.57627868652344%\n",
            "training loss: 0.0003482040620403188; time: 16.68149161338806s\n",
            "training accuracy: 86.49439239501953%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 29%|██▉       | 29/100 [08:54<21:38, 18.29s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "testing loss: 0.014464197678269846; time: 1.480614423751831s\n",
            "testing accuracy: 80.38977813720703%\n",
            "training loss: 0.00033518163054488984; time: 16.867227792739868s\n",
            "training accuracy: 86.68870544433594%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 30%|███       | 30/100 [09:13<21:41, 18.60s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "testing loss: 0.014675950223676574; time: 1.524510383605957s\n",
            "testing accuracy: 80.4830322265625%\n",
            "training loss: 0.00032232649980815445; time: 16.623897552490234s\n",
            "training accuracy: 86.77684020996094%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 31%|███       | 31/100 [09:32<21:16, 18.50s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "testing loss: 0.014608494003665665; time: 1.6253774166107178s\n",
            "testing accuracy: 81.11712646484375%\n",
            "training loss: 0.00031081551947304085; time: 16.778342247009277s\n",
            "training accuracy: 86.7568130493164%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 32%|███▏      | 32/100 [09:50<20:54, 18.44s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "testing loss: 0.014453029185005508; time: 1.5300781726837158s\n",
            "testing accuracy: 80.88399505615234%\n",
            "training loss: 0.0002996251471709712; time: 16.77653408050537s\n",
            "training accuracy: 86.85295867919922%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 33%|███▎      | 33/100 [10:08<20:32, 18.40s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "testing loss: 0.01444818414205804; time: 1.4972858428955078s\n",
            "testing accuracy: 81.09847259521484%\n",
            "training loss: 0.0002905817074770723; time: 16.708003997802734s\n",
            "training accuracy: 86.92107391357422%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 34%|███▍      | 34/100 [10:27<20:11, 18.35s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "testing loss: 0.014411213110572408; time: 1.516857624053955s\n",
            "testing accuracy: 80.4830322265625%\n",
            "training loss: 0.00028008843423293786; time: 16.728912353515625s\n",
            "training accuracy: 87.0252456665039%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 35%|███▌      | 35/100 [10:45<19:53, 18.36s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "testing loss: 0.014187616531982746; time: 1.6525294780731201s\n",
            "testing accuracy: 81.39686584472656%\n",
            "training loss: 0.00025225303697491334; time: 16.699416399002075s\n",
            "training accuracy: 88.0769271850586%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 36%|███▌      | 36/100 [11:03<19:32, 18.31s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "testing loss: 0.013388238724978367; time: 1.4855570793151855s\n",
            "testing accuracy: 82.36665344238281%\n",
            "training loss: 0.00024263251880867948; time: 17.00139880180359s\n",
            "training accuracy: 88.21113586425781%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 37%|███▋      | 37/100 [11:22<19:17, 18.38s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "testing loss: 0.013360777038195737; time: 1.5115885734558105s\n",
            "testing accuracy: 82.11488342285156%\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}