{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Shallow Detectors.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"pQ0m9yaDqUnF"},"source":["# Shallow detectors"]},{"cell_type":"code","metadata":{"id":"lS7elo3dEOZK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618880264332,"user_tz":240,"elapsed":386,"user":{"displayName":"Marta Méndez Simón","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gits5ZctTuGucR808yQ0fvncHIaUqOjK19t0PujGQ=s64","userId":"15648474664899414572"}},"outputId":"876a04d0-bb39-4d8a-a9ec-8f19f567e16c"},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":22,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"bJQF8XwhESvy","executionInfo":{"status":"ok","timestamp":1618880390888,"user_tz":240,"elapsed":375,"user":{"displayName":"Marta Méndez Simón","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gits5ZctTuGucR808yQ0fvncHIaUqOjK19t0PujGQ=s64","userId":"15648474664899414572"}}},"source":["drivepath = '/content/gdrive/MyDrive/CMU/11785_Intro_to_Deep_Learning/DL_Group_Project/Dataset/Preprocessed_Data'"],"execution_count":35,"outputs":[]},{"cell_type":"code","metadata":{"id":"tNu7076tIgbg","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618880268270,"user_tz":240,"elapsed":2516,"user":{"displayName":"Marta Méndez Simón","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gits5ZctTuGucR808yQ0fvncHIaUqOjK19t0PujGQ=s64","userId":"15648474664899414572"}},"outputId":"23ba1782-c1fb-4fbc-c1a5-ef06fd33b498"},"source":["!pip install tqdm"],"execution_count":24,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.41.1)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"a0sSLZGvAcnu","executionInfo":{"status":"ok","timestamp":1618880268271,"user_tz":240,"elapsed":1929,"user":{"displayName":"Marta Méndez Simón","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gits5ZctTuGucR808yQ0fvncHIaUqOjK19t0PujGQ=s64","userId":"15648474664899414572"}}},"source":["import numpy as np\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","import os\n","from tqdm import tqdm\n","import time"],"execution_count":25,"outputs":[]},{"cell_type":"code","metadata":{"id":"toPNjU3GMS7A","executionInfo":{"status":"ok","timestamp":1618881045914,"user_tz":240,"elapsed":272,"user":{"displayName":"Marta Méndez Simón","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gits5ZctTuGucR808yQ0fvncHIaUqOjK19t0PujGQ=s64","userId":"15648474664899414572"}}},"source":["NUM_EPOCHS = 30\n","BATCH_SIZE = 64\n","HIDDEN_SIZE = 128\n","MODEL_VERSION = 1\n","LEARNING_RATE = 0.01\n","LOGISTIC_THRESHOLD = 0.5"],"execution_count":69,"outputs":[]},{"cell_type":"code","metadata":{"id":"HlpeV6MJHvzv","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618880277516,"user_tz":240,"elapsed":230,"user":{"displayName":"Marta Méndez Simón","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gits5ZctTuGucR808yQ0fvncHIaUqOjK19t0PujGQ=s64","userId":"15648474664899414572"}},"outputId":"9fcb1aac-effa-4ac0-ef2e-6ca3d17e8521"},"source":["cuda = torch.cuda.is_available()\n","num_workers = 8 if cuda else 0\n","DEVICE = \"cuda\" if cuda else \"cpu\"\n","print(\"Cuda = \"+str(cuda)+\" with num_workers = \"+str(num_workers))"],"execution_count":27,"outputs":[{"output_type":"stream","text":["Cuda = True with num_workers = 8\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"AOUpENboAOIW","executionInfo":{"status":"ok","timestamp":1618881033202,"user_tz":240,"elapsed":258,"user":{"displayName":"Marta Méndez Simón","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gits5ZctTuGucR808yQ0fvncHIaUqOjK19t0PujGQ=s64","userId":"15648474664899414572"}}},"source":["class PhonemeDataset(Dataset):\n","    \n","    def __init__(self, basepath, phoneme_tag, mode):\n","      # AA.wav\n","      # AA recording -> FRAMES, LABELS\n","      # 70% of FRAMES to train; 20% to dev; 10% to test\n","      # AA_train_features.npy (NUM_FRAMES, 40)\n","      # AA_dev_features.npy\n","      # AA_test_features.npy\n","      # AA_train_labels.npy\n","\n","      # load specific recording features and labels\n","      phoneme_features_path = f\"{basepath}/{phoneme_tag}_{mode}_features.npy\"\n","      phoneme_labels_path = f\"{basepath}/{phoneme_tag}_{mode}_labels.npy\"\n","\n","      phoneme_features = np.load(phoneme_features_path, allow_pickle=True)\n","      phoneme_labels = np.load(phoneme_labels_path, allow_pickle=True)\n","\n","      print(f\"[{phoneme_tag}] phoneme_features.shape: {phoneme_features.shape}\")\n","      print(f\"[{phoneme_tag}] phoneme_labels.shape: {phoneme_labels.shape}\")\n","\n","      phoneme_labels = np.where(phoneme_labels != 0, 1, 0)\n","\n","      # load 30% of other_phoneme=1, do not load silence\n","      # go through all other files in basepath and load them with label=0\n","      with os.scandir(basepath) as entries:\n","        for entry in entries:\n","          if entry.is_file() and phoneme_tag not in entry.name:\n","            if \"features\" in entry.name and mode in entry.name:\n","              other_phoneme_tag = entry.name.split(\"_\")[0]\n","\n","              features_filepath = entry.path\n","              labels_filepath = f\"{basepath}/{other_phoneme_tag}_{mode}_labels.npy\"\n","\n","              other_phoneme_features = np.load(features_filepath, allow_pickle=True)\n","              other_phoneme_labels = np.load(labels_filepath, allow_pickle=True)\n","              \n","              # find frames where label != 0\n","              # non_zero_labels = other\n","              other_phoneme_features[]\n","\n","\n","              # stack to phoneme features\n","              phoneme_features = np.concatenate((phoneme_features, other_phoneme_features))\n","\n","\n","\n","      self.X = phoneme_features\n","      self.Y = phoneme_labels\n","\n","    def __len__(self):\n","        return len(self.X)\n"," \n","    # get a row at an index\n","    def __getitem__(self, index):\n","        x = torch.Tensor(self.X[index]).float()\n","        y = torch.as_tensor(self.Y[index]).float()\n","    \n","        return x,y"],"execution_count":67,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":130},"id":"XnYfqte6xK-6","executionInfo":{"status":"error","timestamp":1618880994821,"user_tz":240,"elapsed":248,"user":{"displayName":"Marta Méndez Simón","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gits5ZctTuGucR808yQ0fvncHIaUqOjK19t0PujGQ=s64","userId":"15648474664899414572"}},"outputId":"24ae35c2-e8fb-435a-da0e-1bf5f09cac37"},"source":["class SilenceDataset(Dataset):\n","    \n","    def __init__(self, basepath, mode):\n","      silence_features = np.zeros((1, 40))  # first row to delete at the end\n","      silence_labels = np.zeros((1))\n","\n","      # go through all other files in basepath and load them with label=0\n","      with os.scandir(basepath) as entries:\n","        for entry in entries:\n","          if entry.is_file() and phoneme_tag not in entry.name:\n","            if \"features\" in entry.name and mode in entry.name:\n","              phoneme_features = np.load(entry.path, allow_pickle=True)\n","              # stack to phoneme features\n","              phoneme_features = np.concatenate((phoneme_features, other_phoneme_features))\n","\n","              # we will set all zeros as labels\n","              other_phoneme_labels = np.zeros((len(other_phoneme_features)))\n","              phoneme_labels = np.concatenate((phoneme_labels, other_phoneme_labels))\n","            if \"labels\" in entry.name and mode in entry.name:\n","              #pass\n","\n","      self.X = phoneme_features\n","      self.Y = phoneme_labels\n","\n","    def __len__(self):\n","        return len(self.X)\n"," \n","    # get a row at an index\n","    def __getitem__(self, index):\n","        x = torch.Tensor(self.X[index]).float()\n","        y = torch.as_tensor(self.Y[index])\n","    \n","        return x,y"],"execution_count":64,"outputs":[{"output_type":"error","ename":"IndentationError","evalue":"ignored","traceback":["\u001b[0;36m  File \u001b[0;32m\"<ipython-input-64-5c0533e02736>\"\u001b[0;36m, line \u001b[0;32m22\u001b[0m\n\u001b[0;31m    self.X = phoneme_features\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"]}]},{"cell_type":"code","metadata":{"id":"psP0nwMNE9yv","executionInfo":{"status":"ok","timestamp":1618880290660,"user_tz":240,"elapsed":363,"user":{"displayName":"Marta Méndez Simón","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gits5ZctTuGucR808yQ0fvncHIaUqOjK19t0PujGQ=s64","userId":"15648474664899414572"}}},"source":["def make_dataloader(dataset, train, batch_size):\n","  if train:\n","    shuffle = True\n","    drop_last = True\n","  else:\n","    shuffle = False\n","    drop_last = False\n","    \n","  loader = DataLoader(dataset=dataset, batch_size=batch_size,\n","                      drop_last=drop_last, shuffle=shuffle,\n","                      pin_memory=True, num_workers=8)\n","  \n","  return loader"],"execution_count":30,"outputs":[]},{"cell_type":"code","metadata":{"id":"--p_S9L8GWRd","executionInfo":{"status":"ok","timestamp":1618880500760,"user_tz":240,"elapsed":229,"user":{"displayName":"Marta Méndez Simón","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gits5ZctTuGucR808yQ0fvncHIaUqOjK19t0PujGQ=s64","userId":"15648474664899414572"}}},"source":["class PhonemeShallowDetector(nn.Module):\n","  \n","  def __init__(self, hidden_size, activation):\n","    super(PhonemeShallowDetector, self).__init__()\n","    \n","    self.linear_layer = nn.Linear(in_features=40, out_features=hidden_size)\n","    self.bn_layer = nn.BatchNorm1d(num_features=hidden_size)\n","    self.activation = activation\n","    self.output_layer = nn.Linear(in_features=hidden_size, out_features=1)\n","    self.sigmoid = nn.Sigmoid()\n","    seq_params = [\n","      self.linear_layer,\n","      self.bn_layer,\n","      self.activation,\n","      self.output_layer,\n","      self.sigmoid\n","    ]\n","\n","    self.network = nn.Sequential(*seq_params)\n","    \n","  def forward(self, x):\n","    return self.network(x)"],"execution_count":42,"outputs":[]},{"cell_type":"code","metadata":{"id":"KNhuSUg0_5sH","executionInfo":{"status":"ok","timestamp":1618882040617,"user_tz":240,"elapsed":495,"user":{"displayName":"Marta Méndez Simón","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gits5ZctTuGucR808yQ0fvncHIaUqOjK19t0PujGQ=s64","userId":"15648474664899414572"}}},"source":["class ShallowDetector():\n","\n","  def __init__(self, phoneme_tag):\n","    self.phoneme_tag = phoneme_tag\n","\n","    train_data = PhonemeDataset(basepath=drivepath, phoneme_tag=phoneme_tag, mode=\"train\")\n","    self.train_loader = make_dataloader(dataset=train_data, train=True, batch_size=BATCH_SIZE)\n","    print(f\"[{phoneme_tag}] train_data.shape: {train_data.X.shape}\")\n","\n","    dev_data = PhonemeDataset(basepath=drivepath, phoneme_tag=phoneme_tag, mode=\"dev\")\n","    self.dev_loader = make_dataloader(dataset=dev_data, train=False, batch_size=BATCH_SIZE)\n","    print(f\"[{phoneme_tag}] dev_data.shape: {dev_data.X.shape}\")\n","\n","    self.model = PhonemeShallowDetector(hidden_size=HIDDEN_SIZE, \n","                                        activation=nn.LeakyReLU()).to(DEVICE)\n","    self.criterion = nn.BCELoss()\n","    self.optimizer = torch.optim.SGD(self.model.parameters(), lr=LEARNING_RATE, momentum=0.9)\n","    self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, 'min')\n","\n","    self.train_loss_per_epoch = []\n","    self.train_acc_per_epoch = []\n","    self.dev_loss_per_epoch = []\n","    self.dev_acc_per_epoch = []\n","  \n","  def save_model(self, epoch):\n","    model_epoch_path = \"{}/shallow_detectors/model_{}_{}_{}\".format(drivepath, \n","                                                                    self.phoneme_tag, \n","                                                                    MODEL_VERSION, \n","                                                                    epoch)\n","    torch.save({\n","        'model_state_dict': self.model.state_dict(),\n","        'optimizer_state_dict': self.optimizer.state_dict(),\n","        'scheduler_state_dict': self.scheduler.state_dict(),\n","    }, model_epoch_path)\n","    # print('saved model: {}'.format(model_epoch_path))\n","\n","  def train(self, epochs):\n","    # Run training and track with wandb\n","    total_batches = len(self.train_loader) * epochs\n","    example_ct = 0  # number of examples seen\n","    batch_ct = 0\n","\n","    for epoch in tqdm(range(epochs)):\n","        train_loss = 0.0\n","        start_time = time.time()\n","        total_predictions = 0\n","        correct_predictions = 0\n","\n","        for _, (features, targets) in enumerate(self.train_loader):\n","            batch_loss, outputs = self.train_batch(features, targets)\n","            train_loss += batch_loss\n","\n","            example_ct += len(features)\n","            batch_ct += 1\n","\n","            targets = targets.reshape(-1, 1)\n","\n","            # check number of correct predictions\n","            output_classes = torch.where(outputs > LOGISTIC_THRESHOLD, 1, 0)  # convert to class labels\n","            total_predictions += len(output_classes)\n","            correct_predictions += torch.sum(targets == output_classes.detach().cpu())\n","\n","        end_time = time.time()\n","\n","        train_loss /= example_ct\n","        print(f\"training loss: {train_loss}; time: {end_time - start_time}s\")\n","        train_acc = (correct_predictions/total_predictions) * 100.0\n","        print(f\"training accuracy: {train_acc}%\")\n","\n","        self.train_loss_per_epoch.append(train_loss)\n","        self.train_acc_per_epoch.append(train_acc)\n","\n","        # evaluate model with validation data\n","        dev_loss, dev_acc = self.evaluate_model()\n","        \n","        self.dev_loss_per_epoch.append(dev_loss)\n","        self.dev_acc_per_epoch.append(dev_acc)\n","\n","        # Step with the scheduler\n","        self.scheduler.step(dev_loss)\n","\n","        # epoch completed, save model\n","        self.save_model(epoch)\n","\n","  def train_batch(self, features, targets):\n","    features, targets = features.to(DEVICE), targets.to(DEVICE)\n","    targets = targets.reshape(-1, 1)\n","\n","    self.optimizer.zero_grad()\n","\n","    # Forward pass ➡\n","    outputs = self.model(features)\n","    loss = self.criterion(outputs, targets)  # compare with target outputs\n","    # Backward pass ⬅\n","    loss.backward()\n","    # Step with optimizer\n","    self.optimizer.step()\n","\n","    return loss.item(), outputs\n","\n","  def evaluate_model(self):\n","\n","    with torch.no_grad():\n","      self.model.eval()\n","\n","      running_loss = 0.0\n","      total_predictions = 0.0\n","      correct_predictions = 0.0\n","\n","      example_ct = 0\n","\n","      start_time = time.time()\n","      for batch_idx, (features, targets) in enumerate(self.dev_loader):\n","        features = features.to(DEVICE)\n","        targets = targets.to(DEVICE)\n","        targets = targets.reshape(-1, 1)\n","\n","        example_ct += len(features)\n","\n","        outputs = self.model(features)\n","        outputs = outputs.to(DEVICE)\n","\n","        # check number of correct predictions\n","        output_classes = torch.where(outputs > LOGISTIC_THRESHOLD, 1, 0)  # convert to class labels\n","        total_predictions += len(output_classes)\n","        correct_predictions += torch.sum(targets == output_classes)\n","\n","        loss = self.criterion(outputs, targets).detach()\n","        running_loss += loss.item()\n","      \n","      end_time = time.time()\n","\n","      running_loss /= example_ct\n","      print(f\"testing loss: {running_loss}; time: {end_time - start_time}s\")\n","      acc = (correct_predictions/total_predictions) * 100.0\n","      print(f\"testing accuracy: {acc}%\")\n","      return running_loss, acc"],"execution_count":105,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hmgukhuSqx7q"},"source":["## 2. Training shallow detectors (per phoneme + one for silence)"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9RgaXKNzySdO","executionInfo":{"status":"ok","timestamp":1618880399376,"user_tz":240,"elapsed":223,"user":{"displayName":"Marta Méndez Simón","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gits5ZctTuGucR808yQ0fvncHIaUqOjK19t0PujGQ=s64","userId":"15648474664899414572"}},"outputId":"dcba02f8-b26f-45c8-d0a8-dad90936ab44"},"source":["%cd /content/gdrive/MyDrive/CMU/11785_Intro_to_Deep_Learning/DL_Group_Project/Dataset/Preprocessed_Data"],"execution_count":36,"outputs":[{"output_type":"stream","text":["/content/gdrive/.shortcut-targets-by-id/1-5qSMUCOdbNlj6D2Mas-EnlePRAw5fu5/11785_Intro_to_Deep_Learning/DL_Group_Project/Dataset/Preprocessed_Data\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"tjbfaEh8yYL1","executionInfo":{"status":"ok","timestamp":1618880403116,"user_tz":240,"elapsed":1605,"user":{"displayName":"Marta Méndez Simón","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gits5ZctTuGucR808yQ0fvncHIaUqOjK19t0PujGQ=s64","userId":"15648474664899414572"}}},"source":["from utilities import PHONEME_MAPPER"],"execution_count":37,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"N7TH0JJh3QiH","executionInfo":{"status":"ok","timestamp":1618881618606,"user_tz":240,"elapsed":224,"user":{"displayName":"Marta Méndez Simón","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gits5ZctTuGucR808yQ0fvncHIaUqOjK19t0PujGQ=s64","userId":"15648474664899414572"}},"outputId":"2e18aeed-471b-429c-e730-be2260795dfa"},"source":["%cd /"],"execution_count":93,"outputs":[{"output_type":"stream","text":["/\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"M3TvLFZBygZ4","executionInfo":{"status":"ok","timestamp":1618880404298,"user_tz":240,"elapsed":249,"user":{"displayName":"Marta Méndez Simón","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gits5ZctTuGucR808yQ0fvncHIaUqOjK19t0PujGQ=s64","userId":"15648474664899414572"}},"outputId":"e2b80c59-5a26-4b06-d1c5-24aa3f874a1b"},"source":["print(PHONEME_MAPPER)"],"execution_count":38,"outputs":[{"output_type":"stream","text":["{0: 'SIL', 1: 'AE', 2: 'AH', 3: 'AW', 4: 'AY', 5: 'B', 6: 'BIT', 7: 'D', 8: 'DH', 9: 'EE', 10: 'FF', 11: 'G', 12: 'HH', 13: 'IH', 14: 'II', 15: 'J', 16: 'K', 17: 'LL', 18: 'MM', 19: 'NN', 20: 'OH', 21: 'OO', 22: 'OW', 23: 'OY', 24: 'P', 25: 'RR', 26: 'SH', 27: 'SS', 28: 'T', 29: 'TH', 30: 'UE', 31: 'UH', 32: 'VV', 33: 'WW', 34: 'YY', 35: 'ZZ'}\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"mvRk8m8CLwGn","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"error","timestamp":1618882064265,"user_tz":240,"elapsed":21549,"user":{"displayName":"Marta Méndez Simón","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gits5ZctTuGucR808yQ0fvncHIaUqOjK19t0PujGQ=s64","userId":"15648474664899414572"}},"outputId":"4f2c5f3e-71a5-40be-b162-7d93ad6341cd"},"source":["for phoneme_index, phoneme_tag in PHONEME_MAPPER.items():\n","  if phoneme_tag == \"SIL\":\n","    continue\n","  detector = ShallowDetector(phoneme_tag)\n","  detector.train(epochs=NUM_EPOCHS)\n","  break"],"execution_count":106,"outputs":[{"output_type":"stream","text":["[AE] phoneme_features.shape: (1438, 40)\n","[AE] phoneme_labels.shape: (1438,)\n","[AE] train_data.shape: (49976, 40)\n","[AE] phoneme_features.shape: (308, 40)\n","[AE] phoneme_labels.shape: (308,)\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n","\n","\n","\n","\n","  0%|          | 0/30 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["[AE] dev_data.shape: (10724, 40)\n","training loss: 0.0005177332491000878; time: 2.192976236343384s\n","training accuracy: 99.18470001220703%\n"],"name":"stdout"},{"output_type":"stream","text":["\n","\n","\n","\n","  3%|▎         | 1/30 [00:02<01:19,  2.76s/it]\u001b[A\u001b[A\u001b[A\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["testing loss: 0.0006671954525628551; time: 0.5540025234222412s\n","testing accuracy: 99.22603607177734%\n","training loss: 0.0002465968442809156; time: 2.1364266872406006s\n","training accuracy: 99.35095977783203%\n"],"name":"stdout"},{"output_type":"stream","text":["\n","\n","\n","\n","  7%|▋         | 2/30 [00:05<01:16,  2.74s/it]\u001b[A\u001b[A\u001b[A\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["testing loss: 0.0008058253783903661; time: 0.5422475337982178s\n","testing accuracy: 99.18873596191406%\n","training loss: 0.00013552397364502896; time: 2.1212968826293945s\n","training accuracy: 99.43710327148438%\n"],"name":"stdout"},{"output_type":"stream","text":["\n","\n","\n","\n"," 10%|█         | 3/30 [00:08<01:13,  2.73s/it]\u001b[A\u001b[A\u001b[A\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["testing loss: 0.0005109972088274305; time: 0.5594527721405029s\n","testing accuracy: 99.31928253173828%\n","training loss: 9.510044630722803e-05; time: 2.158604383468628s\n","training accuracy: 99.46514129638672%\n"],"name":"stdout"},{"output_type":"stream","text":["\n","\n","\n","\n"," 13%|█▎        | 4/30 [00:10<01:11,  2.73s/it]\u001b[A\u001b[A\u001b[A\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["testing loss: 0.0007120125331185475; time: 0.5842471122741699s\n","testing accuracy: 99.17008972167969%\n","training loss: 6.924287536632024e-05; time: 2.1492786407470703s\n","training accuracy: 99.51722717285156%\n"],"name":"stdout"},{"output_type":"stream","text":["\n","\n","\n","\n"," 17%|█▋        | 5/30 [00:13<01:08,  2.73s/it]\u001b[A\u001b[A\u001b[A\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["testing loss: 0.0005447611424104983; time: 0.5656337738037109s\n","testing accuracy: 99.27265930175781%\n","training loss: 5.6270180326443e-05; time: 2.2985050678253174s\n","training accuracy: 99.51722717285156%\n"],"name":"stdout"},{"output_type":"stream","text":["\n","\n","\n","\n"," 20%|██        | 6/30 [00:16<01:06,  2.77s/it]\u001b[A\u001b[A\u001b[A\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["testing loss: 0.00046617617318120115; time: 0.5620584487915039s\n","testing accuracy: 99.34725952148438%\n","training loss: 4.6392637280453424e-05; time: 2.140101909637451s\n","training accuracy: 99.53125%\n"],"name":"stdout"},{"output_type":"stream","text":["\n","\n","\n","\n"," 23%|██▎       | 7/30 [00:19<01:03,  2.76s/it]\u001b[A\u001b[A\u001b[A\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["testing loss: 0.0006475755275122759; time: 0.5605299472808838s\n","testing accuracy: 99.25401306152344%\n"],"name":"stdout"},{"output_type":"stream","text":["Exception in thread Thread-112:\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n","    self.run()\n","  File \"/usr/lib/python3.7/threading.py\", line 870, in run\n","    self._target(*self._args, **self._kwargs)\n","  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/pin_memory.py\", line 25, in _pin_memory_loop\n","    r = in_queue.get(timeout=MP_STATUS_CHECK_INTERVAL)\n","  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 113, in get\n","    return _ForkingPickler.loads(res)\n","  File \"/usr/local/lib/python3.7/dist-packages/torch/multiprocessing/reductions.py\", line 282, in rebuild_storage_fd\n","    fd = df.detach()\n","  File \"/usr/lib/python3.7/multiprocessing/resource_sharer.py\", line 57, in detach\n","    with _resource_sharer.get_connection(self._id) as conn:\n","  File \"/usr/lib/python3.7/multiprocessing/resource_sharer.py\", line 87, in get_connection\n","    c = Client(address, authkey=process.current_process().authkey)\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 498, in Client\n","    answer_challenge(c, authkey)\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 746, in answer_challenge\n","    connection.send_bytes(digest)\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 200, in send_bytes\n","    self._send_bytes(m[offset:offset + size])\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 404, in _send_bytes\n","    self._send(header + buf)\n","  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 368, in _send\n","    n = write(self._handle, buf)\n","BrokenPipeError: [Errno 32] Broken pipe\n","\n"],"name":"stderr"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-106-dd394e9d6e6d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mdetector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mShallowDetector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphoneme_tag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m   \u001b[0mdetector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNUM_EPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m   \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-105-cbcee0c07aff>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, epochs)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m             \u001b[0mbatch_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m             \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbatch_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-105-cbcee0c07aff>\u001b[0m in \u001b[0;36mtrain_batch\u001b[0;34m(self, features, targets)\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;31m# Step with optimizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/profiler.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, exc_type, exc_value, traceback)\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_type\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_value\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    620\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_callbacks_on_exit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 621\u001b[0;31m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_record_function_exit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    622\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    623\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_end_callbacks_on_future\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfut\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mFuture\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mFuture\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]}]}