{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Specialized Task Shallow Detectors.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"pQ0m9yaDqUnF"},"source":["# Specialized Task Shallow Detectors\n","\n","*   Vowel vs Consonant \n","*   Front vowels vs Back vowels\n","*   High vowels vs Low vowels\n","\n","What does Prof. Baker mean with:\n","*   distinguish manner-of-articulation for consonants\n","*   place of articulation of consonants\n","\n"]},{"cell_type":"code","metadata":{"id":"lS7elo3dEOZK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619819071316,"user_tz":240,"elapsed":52618,"user":{"displayName":"Marta Mendez Simon","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg9EqCHVofgov1UnIhH92rSm6vEcl3y0I6ofEhT=s64","userId":"09823525483180080942"}},"outputId":"45ad8236-bfb0-4b4b-a99c-3095c3d2ea05"},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"bJQF8XwhESvy","executionInfo":{"status":"ok","timestamp":1619819422200,"user_tz":240,"elapsed":323,"user":{"displayName":"Marta Mendez Simon","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg9EqCHVofgov1UnIhH92rSm6vEcl3y0I6ofEhT=s64","userId":"09823525483180080942"}}},"source":["drivepath = '/content/gdrive/MyDrive/Spring_2021/11785_Intro_to_Deep_Learning/DL_Group_Project'\n","datapath = '/content/gdrive/MyDrive/Spring_2021/11785_Intro_to_Deep_Learning/dl-group-project-code/preprocessed_data'\n","output_path = f'{drivepath}/experiments/specialized_detectors/'"],"execution_count":43,"outputs":[]},{"cell_type":"code","metadata":{"id":"tNu7076tIgbg","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619819426454,"user_tz":240,"elapsed":3792,"user":{"displayName":"Marta Mendez Simon","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg9EqCHVofgov1UnIhH92rSm6vEcl3y0I6ofEhT=s64","userId":"09823525483180080942"}},"outputId":"1b2861c8-63a7-47c1-a5ef-5c742db7bcf9"},"source":["!pip install tqdm"],"execution_count":44,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.41.1)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"a0sSLZGvAcnu","executionInfo":{"status":"ok","timestamp":1619819426455,"user_tz":240,"elapsed":3628,"user":{"displayName":"Marta Mendez Simon","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg9EqCHVofgov1UnIhH92rSm6vEcl3y0I6ofEhT=s64","userId":"09823525483180080942"}}},"source":["import numpy as np\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","import os\n","from tqdm import tqdm\n","import time\n","from sklearn.metrics import classification_report\n","from sklearn.metrics import confusion_matrix\n","import pandas as pd"],"execution_count":45,"outputs":[]},{"cell_type":"code","metadata":{"id":"toPNjU3GMS7A","executionInfo":{"status":"ok","timestamp":1619819426456,"user_tz":240,"elapsed":3471,"user":{"displayName":"Marta Mendez Simon","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg9EqCHVofgov1UnIhH92rSm6vEcl3y0I6ofEhT=s64","userId":"09823525483180080942"}}},"source":["NUM_EPOCHS = 30\n","BATCH_SIZE = 64\n","HIDDEN_SIZE = 128\n","MODEL_VERSION = 1\n","LEARNING_RATE = 0.01\n","LOGISTIC_THRESHOLD = 0.5"],"execution_count":46,"outputs":[]},{"cell_type":"code","metadata":{"id":"HlpeV6MJHvzv","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619819426456,"user_tz":240,"elapsed":3296,"user":{"displayName":"Marta Mendez Simon","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg9EqCHVofgov1UnIhH92rSm6vEcl3y0I6ofEhT=s64","userId":"09823525483180080942"}},"outputId":"1d14166a-1a8f-4834-a51f-e9559f8876ac"},"source":["cuda = torch.cuda.is_available()\n","num_workers = 8 if cuda else 0\n","DEVICE = \"cuda\" if cuda else \"cpu\"\n","print(\"Cuda = \"+str(cuda)+\" with num_workers = \"+str(num_workers))"],"execution_count":47,"outputs":[{"output_type":"stream","text":["Cuda = True with num_workers = 8\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"XnYfqte6xK-6","executionInfo":{"status":"ok","timestamp":1619819460257,"user_tz":240,"elapsed":227,"user":{"displayName":"Marta Mendez Simon","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg9EqCHVofgov1UnIhH92rSm6vEcl3y0I6ofEhT=s64","userId":"09823525483180080942"}}},"source":["class SpecializedDataset(Dataset):\n","    \n","    def __init__(self, datapath, mode, task_name, phonemes_class_0, phonemes_class_1):\n","      \"\"\"\n","      phonemes_class_0: list of phoneme names for class 0\n","      phonemes_class_1: list of phoneme names for class 1\n","      \"\"\"\n","      complete_features = np.zeros((1, 40))  # eliminate this row\n","      complete_labels = np.zeros((1))  # eliminate this row\n","\n","      # go through all files in datapath, check phoneme if class=0 or class=1\n","      # assign that label and discard silence frames\n","      with os.scandir(datapath) as entries:\n","        for entry in entries:\n","          if entry.is_file():\n","            if \"features\" in entry.name and mode in entry.name:\n","              phoneme_tag = entry.name.split(\"_\")[0]\n","\n","              features_filepath = entry.path\n","              labels_filepath = f\"{datapath}/{phoneme_tag}_{mode}_labels.npy\"\n","\n","              phoneme_features = np.load(features_filepath, allow_pickle=True)\n","              phoneme_labels = np.load(labels_filepath, allow_pickle=True)\n","              print(f\"{phoneme_tag} total features: {phoneme_features.shape}\")\n","              print(f\"{phoneme_labels} total labels: {phoneme_labels.shape}\")\n","\n","              # find frames where label != 0 (non-silence)\n","              non_zero_indexes = phoneme_labels.nonzero()\n","              phoneme_features = phoneme_features[non_zero_indexes]\n","              phoneme_labels = phoneme_labels[non_zero_indexes]\n","              print(f\"{phoneme_tag} no-silence features: {phoneme_features.shape}\")\n","              print(f\"{phoneme_labels} no-silence labels: {phoneme_labels.shape}\")\n","\n","              # find phoneme in class_0 or class_1 list and assign label\n","              phoneme_class = None\n","              if phoneme_tag in phonemes_class_0:\n","                phoneme_class = 0\n","              if phoneme_tag in phonemes_class_1:\n","                phoneme_class = 1\n","\n","              if phoneme_class is None:\n","                raise Exception(f\"phoneme '{phoneme_tag}' not found on class 0 nor class 1 lists\")\n","\n","              phoneme_labels[:] = phoneme_class  # label=class\n","\n","              # stack to phoneme features\n","              complete_features = np.concatenate((complete_features, phoneme_features))\n","              complete_labels = np.concatenate((complete_labels, phoneme_labels))\n","      \n","      self.X = complete_features[1:]\n","      self.Y = complete_labels[1:]\n","      print(f\"[task={task_name}] {self.X.shape} features\")\n","      print(f\"[task={task_name}] {self.Y.shape} labels\")\n","\n","    def __len__(self):\n","        return len(self.X)\n"," \n","    # get a row at an index\n","    def __getitem__(self, index):\n","        x = torch.Tensor(self.X[index]).float()\n","        y = torch.as_tensor(self.Y[index]).float()\n","    \n","        return x,y"],"execution_count":66,"outputs":[]},{"cell_type":"code","metadata":{"id":"psP0nwMNE9yv","executionInfo":{"status":"ok","timestamp":1619819460937,"user_tz":240,"elapsed":210,"user":{"displayName":"Marta Mendez Simon","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg9EqCHVofgov1UnIhH92rSm6vEcl3y0I6ofEhT=s64","userId":"09823525483180080942"}}},"source":["def make_dataloader(dataset, train, batch_size):\n","  if train:\n","    shuffle = True\n","    drop_last = True\n","  else:\n","    shuffle = False\n","    drop_last = False\n","    \n","  loader = DataLoader(dataset=dataset, batch_size=batch_size,\n","                      drop_last=drop_last, shuffle=shuffle,\n","                      pin_memory=True, num_workers=8)\n","  \n","  return loader"],"execution_count":67,"outputs":[]},{"cell_type":"code","metadata":{"id":"--p_S9L8GWRd","executionInfo":{"status":"ok","timestamp":1619819461601,"user_tz":240,"elapsed":511,"user":{"displayName":"Marta Mendez Simon","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg9EqCHVofgov1UnIhH92rSm6vEcl3y0I6ofEhT=s64","userId":"09823525483180080942"}}},"source":["class SpecializedShallowDetector(nn.Module):\n","  \n","  def __init__(self, hidden_size, activation):\n","    super(PhonemeShallowDetector, self).__init__()\n","    \n","    self.linear_layer = nn.Linear(in_features=40, out_features=hidden_size)\n","    self.bn_layer = nn.BatchNorm1d(num_features=hidden_size)\n","    self.activation = activation\n","    self.output_layer = nn.Linear(in_features=hidden_size, out_features=1)\n","    self.sigmoid = nn.Sigmoid()\n","    seq_params = [\n","      self.linear_layer,\n","      self.bn_layer,\n","      self.activation,\n","      self.output_layer,\n","      self.sigmoid\n","    ]\n","\n","    self.network = nn.Sequential(*seq_params)\n","    \n","  def forward(self, x):\n","    return self.network(x)"],"execution_count":68,"outputs":[]},{"cell_type":"code","metadata":{"id":"KNhuSUg0_5sH","executionInfo":{"status":"ok","timestamp":1619819461880,"user_tz":240,"elapsed":558,"user":{"displayName":"Marta Mendez Simon","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg9EqCHVofgov1UnIhH92rSm6vEcl3y0I6ofEhT=s64","userId":"09823525483180080942"}}},"source":["class SpecializedDetector():\n","\n","  def __init__(self, task_name, phonemes_class_0, phonemes_class_1):\n","    self.task_name = task_name\n","\n","    train_data = SpecializedDataset(datapath=datapath, mode=\"train\", \n","                                    task_name=task_name, \n","                                    phonemes_class_0=phonemes_class_0, \n","                                    phonemes_class_1=phonemes_class_1)\n","    self.train_loader = make_dataloader(dataset=train_data, train=True, batch_size=BATCH_SIZE)\n","\n","    dev_data = SpecializedDataset(datapath=datapath, mode=\"dev\", \n","                                  task_name=task_name, \n","                                  phonemes_class_0=phonemes_class_0, \n","                                  phonemes_class_1=phonemes_class_1)\n","    self.dev_loader = make_dataloader(dataset=dev_data, train=False, batch_size=BATCH_SIZE)\n","\n","    self.model = SpecializedShallowDetector(hidden_size=HIDDEN_SIZE, \n","                                            activation=nn.LeakyReLU()).to(DEVICE)\n","    self.criterion = nn.BCELoss()\n","    self.optimizer = torch.optim.SGD(self.model.parameters(), lr=LEARNING_RATE, momentum=0.9)\n","    self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, 'min')\n","\n","    self.train_loss_per_epoch = []\n","    self.train_acc_per_epoch = []\n","    self.dev_loss_per_epoch = []\n","    self.dev_acc_per_epoch = []\n","  \n","  def save_model(self, epoch):\n","    model_epoch_path = \"{}/models/model_{}_{}_{}\".format(output_path, self.task_name, \n","                                                         MODEL_VERSION, epoch)\n","    torch.save({\n","        'model_state_dict': self.model.state_dict(),\n","        'optimizer_state_dict': self.optimizer.state_dict(),\n","        'scheduler_state_dict': self.scheduler.state_dict(),\n","    }, model_epoch_path)\n","    # print('saved model: {}'.format(model_epoch_path))\n","\n","  def train(self, epochs):\n","    # Run training and track with wandb\n","    total_batches = len(self.train_loader) * epochs\n","    example_ct = 0  # number of examples seen\n","    batch_ct = 0\n","\n","    for epoch in tqdm(range(epochs)):\n","        train_loss = 0.0\n","        start_time = time.time()\n","        total_predictions = 0\n","        correct_predictions = 0\n","\n","        true_labels = []\n","        predictions = []\n","        for _, (features, targets) in enumerate(self.train_loader):\n","            batch_loss, outputs = self.train_batch(features, targets)\n","            train_loss += batch_loss\n","\n","            example_ct += len(features)\n","            batch_ct += 1\n","\n","            targets = targets.reshape(-1, 1)\n","\n","            # check number of correct predictions\n","            output_classes = torch.where(outputs > LOGISTIC_THRESHOLD, 1, 0).detach().cpu() # convert to class labels\n","            total_predictions += len(output_classes)\n","            correct_predictions += torch.sum(targets == output_classes)\n","\n","            true_labels += list(targets)\n","            predictions += list(output_classes)\n","\n","        end_time = time.time()\n","\n","        train_loss /= example_ct\n","        print(f\"training loss: {train_loss}; time: {end_time - start_time}s\")\n","        \n","        if (epoch + 1) % 10 == 0 or epoch == (epochs - 1):\n","          report = classification_report(true_labels, predictions, output_dict=True)\n","          df = pd.DataFrame(report).transpose()\n","          df.to_csv(f\"{output_path}/reports/train_{self.task_name}_{MODEL_VERSION}_{epoch + 1}.csv\", index=False)\n","\n","        print(classification_report(true_labels, predictions))\n","        print(confusion_matrix(true_labels, predictions))\n","\n","        self.train_loss_per_epoch.append(train_loss)\n","        #self.train_acc_per_epoch.append(train_acc)\n","\n","        # evaluate model with validation data\n","        dev_loss = self.evaluate_model(epoch)\n","        \n","        self.dev_loss_per_epoch.append(dev_loss)\n","        #self.dev_acc_per_epoch.append(dev_acc)\n","\n","        # Step with the scheduler\n","        self.scheduler.step(dev_loss)\n","      \n","    # epoch completed, save model\n","    self.save_model(epoch)\n","\n","  def train_batch(self, features, targets):\n","    features, targets = features.to(DEVICE), targets.to(DEVICE)\n","    targets = targets.reshape(-1, 1)\n","\n","    self.optimizer.zero_grad()\n","\n","    # Forward pass ➡\n","    outputs = self.model(features)\n","    loss = self.criterion(outputs, targets)  # compare with target outputs\n","    # Backward pass ⬅\n","    loss.backward()\n","    # Step with optimizer\n","    self.optimizer.step()\n","\n","    return loss.item(), outputs\n","\n","  def evaluate_model(self, epoch):\n","\n","    with torch.no_grad():\n","      self.model.eval()\n","\n","      running_loss = 0.0\n","      total_predictions = 0.0\n","      correct_predictions = 0.0\n","\n","      true_labels = []\n","      predictions = []\n","\n","      example_ct = 0\n","      start_time = time.time()\n","      for batch_idx, (features, targets) in enumerate(self.dev_loader):\n","        features = features.to(DEVICE)\n","        targets = targets.to(DEVICE)\n","        targets = targets.reshape(-1, 1)\n","\n","        example_ct += len(features)\n","\n","        outputs = self.model(features)\n","        outputs = outputs.to(DEVICE)\n","\n","        # check number of correct predictions\n","        output_classes = torch.where(outputs > LOGISTIC_THRESHOLD, 1, 0)  # convert to class labels\n","        total_predictions += len(output_classes)\n","        correct_predictions += torch.sum(targets == output_classes)\n","\n","        loss = self.criterion(outputs, targets).detach()\n","        running_loss += loss.item()\n","\n","        true_labels += list(targets.detach().cpu())\n","        predictions += list(output_classes.detach().cpu())\n","      \n","      end_time = time.time()\n","\n","      running_loss /= example_ct\n","      print(f\"testing loss: {running_loss}; time: {end_time - start_time}s\")\n","\n","      if (epoch + 1) % 10 == 0:\n","        report = classification_report(true_labels, predictions, output_dict=True)\n","        df = pd.DataFrame(report).transpose()\n","        df.to_csv(f\"{output_path}/reports/reports_dev_{self.task_name}_{MODEL_VERSION}_{epoch + 1}.csv\", index=False)\n","  \n","      print(classification_report(true_labels, predictions))\n","      print(confusion_matrix(true_labels, predictions))\n","      return running_loss"],"execution_count":69,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hmgukhuSqx7q"},"source":["## 2. Training shallow detectors (per phoneme + one for silence)"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9RgaXKNzySdO","executionInfo":{"status":"ok","timestamp":1619819462362,"user_tz":240,"elapsed":568,"user":{"displayName":"Marta Mendez Simon","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg9EqCHVofgov1UnIhH92rSm6vEcl3y0I6ofEhT=s64","userId":"09823525483180080942"}},"outputId":"d91df886-05c8-48d9-f45f-309aea182dbb"},"source":["%cd /content/gdrive/MyDrive/Spring_2021/11785_Intro_to_Deep_Learning/dl-group-project-code/utilities"],"execution_count":70,"outputs":[{"output_type":"stream","text":["/content/gdrive/MyDrive/Spring_2021/11785_Intro_to_Deep_Learning/dl-group-project-code/utilities\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"tjbfaEh8yYL1","executionInfo":{"status":"ok","timestamp":1619819462362,"user_tz":240,"elapsed":340,"user":{"displayName":"Marta Mendez Simon","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg9EqCHVofgov1UnIhH92rSm6vEcl3y0I6ofEhT=s64","userId":"09823525483180080942"}}},"source":["from utilities import SPECIALIZED_TASKS"],"execution_count":71,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"N7TH0JJh3QiH","executionInfo":{"status":"ok","timestamp":1619819463843,"user_tz":240,"elapsed":1553,"user":{"displayName":"Marta Mendez Simon","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg9EqCHVofgov1UnIhH92rSm6vEcl3y0I6ofEhT=s64","userId":"09823525483180080942"}},"outputId":"2c960d5c-6650-4813-9411-ad5a552083df"},"source":["%cd /"],"execution_count":72,"outputs":[{"output_type":"stream","text":["/\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"M3TvLFZBygZ4","executionInfo":{"status":"ok","timestamp":1619819464268,"user_tz":240,"elapsed":247,"user":{"displayName":"Marta Mendez Simon","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg9EqCHVofgov1UnIhH92rSm6vEcl3y0I6ofEhT=s64","userId":"09823525483180080942"}},"outputId":"9606bb17-090a-401c-ab71-189f39a03579"},"source":["print(SPECIALIZED_TASKS)"],"execution_count":73,"outputs":[{"output_type":"stream","text":["{'vowel_vs_consonant': {0: ['AE', 'AH'], 1: []}, 'frontvowel_vs_backvowel': {0: [], 1: []}, 'highvowel_vs_lowvowel': {0: [], 1: ['AH']}}\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"mvRk8m8CLwGn","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"error","timestamp":1619819466547,"user_tz":240,"elapsed":1851,"user":{"displayName":"Marta Mendez Simon","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg9EqCHVofgov1UnIhH92rSm6vEcl3y0I6ofEhT=s64","userId":"09823525483180080942"}},"outputId":"724889ea-002c-400c-9bc1-5bd98791b1bf"},"source":["for task_name, classes_dict in SPECIALIZED_TASKS.items():\n","  detector = SpecializedDetector(task_name, classes_dict[0], classes_dict[1])\n","  detector.train(epochs=NUM_EPOCHS)"],"execution_count":74,"outputs":[{"output_type":"stream","text":["AE total features: (1438, 40)\n","[0 0 0 ... 1 1 1] total labels: (1438,)\n","AE no-silence features: (548, 40)\n","[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"," 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"," 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"," 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"," 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"," 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"," 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"," 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"," 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"," 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"," 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"," 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"," 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"," 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"," 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1] no-silence labels: (548,)\n","AH total features: (1497, 40)\n","[0 0 0 ... 2 2 2] total labels: (1497,)\n","AH no-silence features: (593, 40)\n","[2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n"," 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n"," 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n"," 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n"," 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n"," 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n"," 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n"," 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n"," 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n"," 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n"," 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n"," 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n"," 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n"," 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n"," 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n"," 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n"," 2] no-silence labels: (593,)\n","AW total features: (1480, 40)\n","[0 0 0 ... 3 3 3] total labels: (1480,)\n","AW no-silence features: (608, 40)\n","[3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n"," 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n"," 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n"," 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n"," 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n"," 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n"," 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n"," 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n"," 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n"," 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n"," 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n"," 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n"," 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n"," 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n"," 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n"," 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n"," 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3] no-silence labels: (608,)\n"],"name":"stdout"},{"output_type":"error","ename":"Exception","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-74-ae78cecb73bf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtask_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasses_dict\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mSPECIALIZED_TASKS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0mdetector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSpecializedDetector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasses_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasses_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m   \u001b[0mdetector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNUM_EPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-69-913451149bc5>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, task_name, phonemes_class_0, phonemes_class_1)\u001b[0m\n\u001b[1;32m      7\u001b[0m                                     \u001b[0mtask_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtask_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m                                     \u001b[0mphonemes_class_0\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mphonemes_class_0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m                                     phonemes_class_1=phonemes_class_1)\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_dataloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-66-44b5ee014f74>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, datapath, mode, task_name, phonemes_class_0, phonemes_class_1)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mphoneme_class\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"phoneme '{phoneme_tag}' not found on class 0 nor class 1 lists\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m               \u001b[0mphoneme_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mphoneme_class\u001b[0m  \u001b[0;31m# label=class\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mException\u001b[0m: phoneme 'AW' not found on class 0 nor class 1 lists"]}]},{"cell_type":"code","metadata":{"id":"UmoMuqgewEUo","executionInfo":{"status":"ok","timestamp":1619819402175,"user_tz":240,"elapsed":234,"user":{"displayName":"Marta Mendez Simon","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg9EqCHVofgov1UnIhH92rSm6vEcl3y0I6ofEhT=s64","userId":"09823525483180080942"}}},"source":[""],"execution_count":42,"outputs":[]}]}